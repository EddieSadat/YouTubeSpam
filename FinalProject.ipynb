{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3dfd0b-3d9b-40bd-a01b-129d76c58340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc,precision_recall_curve,roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f8db56a4-2d61-4572-bf32-9b59b63388be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>z13th1q4yzihf1bll23qxzpjeujterydj</td>\n",
       "      <td>Carmen Racasanu</td>\n",
       "      <td>2014-11-14T13:27:52</td>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k</td>\n",
       "      <td>diego mogrovejo</td>\n",
       "      <td>2014-11-14T13:28:08</td>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>z130zd5b3titudkoe04ccbeohojxuzppvbg</td>\n",
       "      <td>BlueYetiPlayz -Call Of Duty and More</td>\n",
       "      <td>2015-05-23T13:04:32</td>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>z12he50arvrkivl5u04cctawgxzkjfsjcc4</td>\n",
       "      <td>Photo Editor</td>\n",
       "      <td>2015-06-05T14:14:48</td>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k</td>\n",
       "      <td>Ray Benich</td>\n",
       "      <td>2015-06-05T18:05:16</td>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID  \\\n",
       "0    LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU   \n",
       "1    LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A   \n",
       "2    LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8   \n",
       "3            z13jhp0bxqncu512g22wvzkasxmvvzjaz04   \n",
       "4            z13fwbwp1oujthgqj04chlngpvzmtt3r3dw   \n",
       "..                                           ...   \n",
       "345            z13th1q4yzihf1bll23qxzpjeujterydj   \n",
       "346        z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k   \n",
       "347          z130zd5b3titudkoe04ccbeohojxuzppvbg   \n",
       "348          z12he50arvrkivl5u04cctawgxzkjfsjcc4   \n",
       "349        z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k   \n",
       "\n",
       "                                   AUTHOR                 DATE  \\\n",
       "0                               Julius NM  2013-11-07T06:20:48   \n",
       "1                             adam riyati  2013-11-07T12:37:15   \n",
       "2                        Evgeny Murashkin  2013-11-08T17:34:21   \n",
       "3                         ElNino Melendez  2013-11-09T08:28:43   \n",
       "4                                  GsMega  2013-11-10T16:05:38   \n",
       "..                                    ...                  ...   \n",
       "345                       Carmen Racasanu  2014-11-14T13:27:52   \n",
       "346                       diego mogrovejo  2014-11-14T13:28:08   \n",
       "347  BlueYetiPlayz -Call Of Duty and More  2015-05-23T13:04:32   \n",
       "348                          Photo Editor  2015-06-05T14:14:48   \n",
       "349                            Ray Benich  2015-06-05T18:05:16   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "0    Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1    Hey guys check out my new channel and our firs...      1  \n",
       "2               just for test I have to say murdev.com      1  \n",
       "3     me shaking my sexy ass on my channel enjoy ^_^ ﻿      1  \n",
       "4              watch?v=vtaRGgvGtWQ   Check this out .﻿      1  \n",
       "..                                                 ...    ...  \n",
       "345  How can this have 2 billion views when there's...      0  \n",
       "346         I don't now why I'm watching this in 2014﻿      0  \n",
       "347  subscribe to me for call of duty vids and give...      1  \n",
       "348  hi guys please my android photo editor downloa...      1  \n",
       "349  The first billion viewed this because they tho...      0  \n",
       "\n",
       "[350 rows x 5 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Opening all csv's\n",
    "df1 = pd.read_csv('Youtube01-Psy.csv')\n",
    "df2 = pd.read_csv('Youtube02-KatyPerry.csv')\n",
    "df3 = pd.read_csv('Youtube03-LMFAO.csv')\n",
    "df4 = pd.read_csv('Youtube04-Eminem.csv')\n",
    "df5 = pd.read_csv('Youtube05-Shakira.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6e42a9-67ef-4a67-b29a-eb0fd4ebae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1956\n"
     ]
    }
   ],
   "source": [
    "#Dropping unwanted columns\n",
    "df1 = df1.drop(columns = ['COMMENT_ID', 'DATE'])\n",
    "df2 = df2.drop(columns = ['COMMENT_ID', 'DATE'])\n",
    "df3 = df3.drop(columns = ['COMMENT_ID', 'DATE'])\n",
    "df4 = df4.drop(columns = ['COMMENT_ID', 'DATE'])\n",
    "df5 = df5.drop(columns = ['COMMENT_ID', 'DATE'])\n",
    "\n",
    "print(len(df1)+len(df2)+len(df3)+len(df4)+len(df5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfcf48a-2055-4aa3-8669-60df9240f8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adam riyati</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUTHOR                                            CONTENT  \\\n",
       "0                Julius NM  Huh, anyway check out this you[tube] channel: ...   \n",
       "1              adam riyati  Hey guys check out my new channel and our firs...   \n",
       "2         Evgeny Murashkin             just for test I have to say murdev.com   \n",
       "3          ElNino Melendez   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4                   GsMega            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "...                    ...                                                ...   \n",
       "1951          Katie Mettam  I love this song because we sing it at Camp al...   \n",
       "1952  Sabina Pearson-Smith  I love this song for two reasons: 1.it is abou...   \n",
       "1953         jeffrey jules                                                wow   \n",
       "1954        Aishlin Maciel                            Shakira u are so wiredo   \n",
       "1955           Latin Bosch                         Shakira is the best dancer   \n",
       "\n",
       "      CLASS  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "1951      0  \n",
       "1952      0  \n",
       "1953      0  \n",
       "1954      0  \n",
       "1955      0  \n",
       "\n",
       "[1956 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combining all df\n",
    "dfAll = pd.concat([df1,df2,df3,df4,df5], axis = 0)\n",
    "len(dfAll)\n",
    "dfAll = dfAll.reset_index().drop('index', axis=1)\n",
    "dfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce60f2d-ff63-40ce-81f2-a13f24a5634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking target class distribution - looks healthy!\n",
    "dfAll['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c26317c-0a8a-463f-bd14-d8e87feda1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to count special characters in a list\n",
    "def special(_list):\n",
    "    special_characters = ['[',']',':',';','<','>','@','#','=','$','%','^','&','*','\\\\','/','!','?','+','-']\n",
    "    count_list = []\n",
    "    for l in _list:\n",
    "        count = 0\n",
    "        for c in l:\n",
    "            if c in special_characters:\n",
    "                count+=1\n",
    "        count_list.append(count)\n",
    "    return(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7edca19-51a8-4dbf-a765-6b4b60ca3f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>C_LEN</th>\n",
       "      <th>A_LEN</th>\n",
       "      <th>C_SPEC</th>\n",
       "      <th>A_SPEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adam riyati</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUTHOR                                            CONTENT  \\\n",
       "0                Julius NM  Huh, anyway check out this you[tube] channel: ...   \n",
       "1              adam riyati  Hey guys check out my new channel and our firs...   \n",
       "2         Evgeny Murashkin             just for test I have to say murdev.com   \n",
       "3          ElNino Melendez   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4                   GsMega            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "...                    ...                                                ...   \n",
       "1951          Katie Mettam  I love this song because we sing it at Camp al...   \n",
       "1952  Sabina Pearson-Smith  I love this song for two reasons: 1.it is abou...   \n",
       "1953         jeffrey jules                                                wow   \n",
       "1954        Aishlin Maciel                            Shakira u are so wiredo   \n",
       "1955           Latin Bosch                         Shakira is the best dancer   \n",
       "\n",
       "      CLASS  C_LEN  A_LEN  C_SPEC  A_SPEC  \n",
       "0         1     56      9       3       0  \n",
       "1         1    166     11       7       0  \n",
       "2         1     38     16       0       0  \n",
       "3         1     48     15       2       0  \n",
       "4         1     39      6       2       0  \n",
       "...     ...    ...    ...     ...     ...  \n",
       "1951      0     58     12       2       0  \n",
       "1952      0     93     20       1       1  \n",
       "1953      0      3     13       0       0  \n",
       "1954      0     23     14       0       0  \n",
       "1955      0     26     11       0       0  \n",
       "\n",
       "[1956 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating new df columns for paramters\n",
    "dfAll['C_LEN'] = [len(i) for i in dfAll['CONTENT']] #character length of comment\n",
    "dfAll['A_LEN'] = [len(i) for i in dfAll['AUTHOR']] #character length of author name\n",
    "dfAll['C_SPEC'] = special(dfAll['CONTENT']) #number of special characters in comment\n",
    "dfAll['A_SPEC'] = special(dfAll['AUTHOR']) #number of special characters in author name\n",
    "\n",
    "dfAll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3fadf-c043-4f45-863d-50d14e117e52",
   "metadata": {
    "tags": []
   },
   "source": [
    "PART 1 - CUSTOM PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed243e86-c466-4f59-85be-b45a93b2dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing variables for training\n",
    "X=dfAll.drop(columns = ['AUTHOR','CONTENT','CLASS'])\n",
    "y=dfAll['CLASS']\n",
    "\n",
    "#Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af299f2-e0eb-4d67-9eba-d9c40c3eae83",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using kernel linear, C value is 0.01, the acc is 0.641\n",
      "[[249  52]\n",
      " [159 127]]\n",
      "using kernel linear, C value is 0.1, the acc is 0.641\n",
      "[[249  52]\n",
      " [159 127]]\n",
      "using kernel linear, C value is 0.5, the acc is 0.642\n",
      "[[247  54]\n",
      " [156 130]]\n",
      "using kernel linear, C value is 1, the acc is 0.642\n",
      "[[247  54]\n",
      " [156 130]]\n",
      "using kernel linear, C value is 5, the acc is 0.641\n",
      "[[246  55]\n",
      " [156 130]]\n",
      "using kernel linear, C value is 10, the acc is 0.641\n",
      "[[246  55]\n",
      " [156 130]]\n",
      "using kernel rbf, C value is 0.01, the acc is 0.627\n",
      "[[254  47]\n",
      " [172 114]]\n",
      "using kernel rbf, C value is 0.1, the acc is 0.606\n",
      "[[218  83]\n",
      " [148 138]]\n",
      "using kernel rbf, C value is 0.5, the acc is 0.617\n",
      "[[188 113]\n",
      " [112 174]]\n",
      "using kernel rbf, C value is 1, the acc is 0.618\n",
      "[[184 117]\n",
      " [107 179]]\n",
      "using kernel rbf, C value is 5, the acc is 0.612\n",
      "[[167 134]\n",
      " [ 94 192]]\n",
      "using kernel rbf, C value is 10, the acc is 0.606\n",
      "[[156 145]\n",
      " [ 86 200]]\n",
      "using kernel poly, C value is 0.01, the acc is 0.596\n",
      "[[296   5]\n",
      " [232  54]]\n",
      "using kernel poly, C value is 0.1, the acc is 0.600\n",
      "[[294   7]\n",
      " [228  58]]\n",
      "using kernel poly, C value is 0.5, the acc is 0.608\n",
      "[[292   9]\n",
      " [221  65]]\n",
      "using kernel poly, C value is 1, the acc is 0.622\n",
      "[[291  10]\n",
      " [212  74]]\n",
      "using kernel poly, C value is 5, the acc is 0.630\n",
      "[[288  13]\n",
      " [204  82]]\n",
      "using kernel poly, C value is 10, the acc is 0.624\n",
      "[[286  15]\n",
      " [206  80]]\n"
     ]
    }
   ],
   "source": [
    "#SVC: Testing several kernal types and C values\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.01, 0.1, 0.5, 1, 5, 10]\n",
    "for kernel in kernels: \n",
    "    for c_val in C_values: \n",
    "        model = SVC(kernel=kernel, C=c_val, probability = True, random_state=10)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        print(\"using kernel {}\".format(kernel) + \", C value is {}\".format(c_val) +\n",
    "              \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "        print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425ba7a5-461f-4243-9a29-531891624bde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss hinge,  penalty is l1,  alpha is 0.001, the acc is 0.514\n",
      "[[301   0]\n",
      " [285   1]]\n",
      "0.514480408858603\n",
      "using loss hinge,  penalty is l1,  alpha is 0.1, the acc is 0.652\n",
      "[[122 179]\n",
      " [ 25 261]]\n",
      "0.6524701873935264\n",
      "using loss hinge,  penalty is l1,  alpha is 1, the acc is 0.620\n",
      "[[193 108]\n",
      " [115 171]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is l1,  alpha is 5, the acc is 0.620\n",
      "[[184 117]\n",
      " [106 180]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is l1,  alpha is 10, the acc is 0.613\n",
      "[[217  84]\n",
      " [143 143]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is l2,  alpha is 0.001, the acc is 0.605\n",
      "[[118 183]\n",
      " [ 49 237]]\n",
      "0.6047700170357752\n",
      "using loss hinge,  penalty is l2,  alpha is 0.1, the acc is 0.600\n",
      "[[200 101]\n",
      " [134 152]]\n",
      "0.5996592844974447\n",
      "using loss hinge,  penalty is l2,  alpha is 1, the acc is 0.613\n",
      "[[219  82]\n",
      " [145 141]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is l2,  alpha is 5, the acc is 0.606\n",
      "[[197 104]\n",
      " [127 159]]\n",
      "0.606473594548552\n",
      "using loss hinge,  penalty is l2,  alpha is 10, the acc is 0.612\n",
      "[[224  77]\n",
      " [151 135]]\n",
      "0.6115843270868825\n",
      "using loss hinge,  penalty is None,  alpha is 0.001, the acc is 0.641\n",
      "[[290  11]\n",
      " [200  86]]\n",
      "0.6405451448040886\n",
      "using loss hinge,  penalty is None,  alpha is 0.1, the acc is 0.613\n",
      "[[181 120]\n",
      " [107 179]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is None,  alpha is 1, the acc is 0.610\n",
      "[[208  93]\n",
      " [136 150]]\n",
      "0.6098807495741057\n",
      "using loss hinge,  penalty is None,  alpha is 5, the acc is 0.603\n",
      "[[223  78]\n",
      " [155 131]]\n",
      "0.6030664395229983\n",
      "using loss hinge,  penalty is None,  alpha is 10, the acc is 0.608\n",
      "[[226  75]\n",
      " [155 131]]\n",
      "0.6081771720613288\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.001, the acc is 0.634\n",
      "[[278  23]\n",
      " [192  94]]\n",
      "0.6337308347529813\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.1, the acc is 0.625\n",
      "[[196 105]\n",
      " [115 171]]\n",
      "0.6252129471890971\n",
      "using loss log_loss,  penalty is l1,  alpha is 1, the acc is 0.624\n",
      "[[280  21]\n",
      " [200  86]]\n",
      "0.6235093696763203\n",
      "using loss log_loss,  penalty is l1,  alpha is 5, the acc is 0.613\n",
      "[[166 135]\n",
      " [ 92 194]]\n",
      "0.6132879045996593\n",
      "using loss log_loss,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.001, the acc is 0.622\n",
      "[[158 143]\n",
      " [ 79 207]]\n",
      "0.6218057921635435\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.1, the acc is 0.600\n",
      "[[181 120]\n",
      " [115 171]]\n",
      "0.5996592844974447\n",
      "using loss log_loss,  penalty is l2,  alpha is 1, the acc is 0.639\n",
      "[[280  21]\n",
      " [191  95]]\n",
      "0.6388415672913118\n",
      "using loss log_loss,  penalty is l2,  alpha is 5, the acc is 0.601\n",
      "[[177 124]\n",
      " [110 176]]\n",
      "0.6013628620102215\n",
      "using loss log_loss,  penalty is l2,  alpha is 10, the acc is 0.603\n",
      "[[182 119]\n",
      " [114 172]]\n",
      "0.6030664395229983\n",
      "using loss log_loss,  penalty is None,  alpha is 0.001, the acc is 0.526\n",
      "[[296   5]\n",
      " [273  13]]\n",
      "0.5264054514480409\n",
      "using loss log_loss,  penalty is None,  alpha is 0.1, the acc is 0.618\n",
      "[[186 115]\n",
      " [109 177]]\n",
      "0.6183986371379898\n",
      "using loss log_loss,  penalty is None,  alpha is 1, the acc is 0.634\n",
      "[[279  22]\n",
      " [193  93]]\n",
      "0.6337308347529813\n",
      "using loss log_loss,  penalty is None,  alpha is 5, the acc is 0.612\n",
      "[[220  81]\n",
      " [147 139]]\n",
      "0.6115843270868825\n",
      "using loss log_loss,  penalty is None,  alpha is 10, the acc is 0.603\n",
      "[[230  71]\n",
      " [162 124]]\n",
      "0.6030664395229983\n",
      "using loss log,  penalty is l1,  alpha is 0.001, the acc is 0.634\n",
      "[[278  23]\n",
      " [192  94]]\n",
      "0.6337308347529813\n",
      "using loss log,  penalty is l1,  alpha is 0.1, the acc is 0.625\n",
      "[[196 105]\n",
      " [115 171]]\n",
      "0.6252129471890971\n",
      "using loss log,  penalty is l1,  alpha is 1, the acc is 0.624\n",
      "[[280  21]\n",
      " [200  86]]\n",
      "0.6235093696763203\n",
      "using loss log,  penalty is l1,  alpha is 5, the acc is 0.613\n",
      "[[166 135]\n",
      " [ 92 194]]\n",
      "0.6132879045996593\n",
      "using loss log,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l2,  alpha is 0.001, the acc is 0.622\n",
      "[[158 143]\n",
      " [ 79 207]]\n",
      "0.6218057921635435\n",
      "using loss log,  penalty is l2,  alpha is 0.1, the acc is 0.600\n",
      "[[181 120]\n",
      " [115 171]]\n",
      "0.5996592844974447\n",
      "using loss log,  penalty is l2,  alpha is 1, the acc is 0.639\n",
      "[[280  21]\n",
      " [191  95]]\n",
      "0.6388415672913118\n",
      "using loss log,  penalty is l2,  alpha is 5, the acc is 0.601\n",
      "[[177 124]\n",
      " [110 176]]\n",
      "0.6013628620102215\n",
      "using loss log,  penalty is l2,  alpha is 10, the acc is 0.603\n",
      "[[182 119]\n",
      " [114 172]]\n",
      "0.6030664395229983\n",
      "using loss log,  penalty is None,  alpha is 0.001, the acc is 0.526\n",
      "[[296   5]\n",
      " [273  13]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5264054514480409\n",
      "using loss log,  penalty is None,  alpha is 0.1, the acc is 0.618\n",
      "[[186 115]\n",
      " [109 177]]\n",
      "0.6183986371379898\n",
      "using loss log,  penalty is None,  alpha is 1, the acc is 0.634\n",
      "[[279  22]\n",
      " [193  93]]\n",
      "0.6337308347529813\n",
      "using loss log,  penalty is None,  alpha is 5, the acc is 0.612\n",
      "[[220  81]\n",
      " [147 139]]\n",
      "0.6115843270868825\n",
      "using loss log,  penalty is None,  alpha is 10, the acc is 0.603\n",
      "[[230  71]\n",
      " [162 124]]\n",
      "0.6030664395229983\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.001, the acc is 0.661\n",
      "[[126 175]\n",
      " [ 24 262]]\n",
      "0.6609880749574105\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.1, the acc is 0.610\n",
      "[[214  87]\n",
      " [142 144]]\n",
      "0.6098807495741057\n",
      "using loss modified_huber,  penalty is l1,  alpha is 1, the acc is 0.606\n",
      "[[217  84]\n",
      " [147 139]]\n",
      "0.606473594548552\n",
      "using loss modified_huber,  penalty is l1,  alpha is 5, the acc is 0.627\n",
      "[[248  53]\n",
      " [166 120]]\n",
      "0.626916524701874\n",
      "using loss modified_huber,  penalty is l1,  alpha is 10, the acc is 0.625\n",
      "[[283  18]\n",
      " [202  84]]\n",
      "0.6252129471890971\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.001, the acc is 0.605\n",
      "[[118 183]\n",
      " [ 49 237]]\n",
      "0.6047700170357752\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.1, the acc is 0.596\n",
      "[[161 140]\n",
      " [ 97 189]]\n",
      "0.596252129471891\n",
      "using loss modified_huber,  penalty is l2,  alpha is 1, the acc is 0.595\n",
      "[[180 121]\n",
      " [117 169]]\n",
      "0.5945485519591142\n",
      "using loss modified_huber,  penalty is l2,  alpha is 5, the acc is 0.634\n",
      "[[227  74]\n",
      " [141 145]]\n",
      "0.6337308347529813\n",
      "using loss modified_huber,  penalty is l2,  alpha is 10, the acc is 0.639\n",
      "[[268  33]\n",
      " [179 107]]\n",
      "0.6388415672913118\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.001, the acc is 0.647\n",
      "[[117 184]\n",
      " [ 23 263]]\n",
      "0.6473594548551959\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.1, the acc is 0.613\n",
      "[[178 123]\n",
      " [104 182]]\n",
      "0.6132879045996593\n",
      "using loss modified_huber,  penalty is None,  alpha is 1, the acc is 0.625\n",
      "[[195 106]\n",
      " [114 172]]\n",
      "0.6252129471890971\n",
      "using loss modified_huber,  penalty is None,  alpha is 5, the acc is 0.622\n",
      "[[232  69]\n",
      " [153 133]]\n",
      "0.6218057921635435\n",
      "using loss modified_huber,  penalty is None,  alpha is 10, the acc is 0.612\n",
      "[[229  72]\n",
      " [156 130]]\n",
      "0.6115843270868825\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.001, the acc is 0.612\n",
      "[[174 127]\n",
      " [101 185]]\n",
      "0.6115843270868825\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.1, the acc is 0.608\n",
      "[[158 143]\n",
      " [ 87 199]]\n",
      "0.6081771720613288\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 1, the acc is 0.601\n",
      "[[206  95]\n",
      " [139 147]]\n",
      "0.6013628620102215\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 5, the acc is 0.634\n",
      "[[263  38]\n",
      " [177 109]]\n",
      "0.6337308347529813\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 10, the acc is 0.629\n",
      "[[250  51]\n",
      " [167 119]]\n",
      "0.6286201022146508\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.001, the acc is 0.612\n",
      "[[121 180]\n",
      " [ 48 238]]\n",
      "0.6115843270868825\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.1, the acc is 0.601\n",
      "[[143 158]\n",
      " [ 76 210]]\n",
      "0.6013628620102215\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 1, the acc is 0.605\n",
      "[[205  96]\n",
      " [136 150]]\n",
      "0.6047700170357752\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 5, the acc is 0.634\n",
      "[[256  45]\n",
      " [170 116]]\n",
      "0.6337308347529813\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 10, the acc is 0.627\n",
      "[[253  48]\n",
      " [171 115]]\n",
      "0.626916524701874\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.001, the acc is 0.615\n",
      "[[177 124]\n",
      " [102 184]]\n",
      "0.6149914821124361\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.1, the acc is 0.625\n",
      "[[149 152]\n",
      " [ 68 218]]\n",
      "0.6252129471890971\n",
      "using loss squared_hinge,  penalty is None,  alpha is 1, the acc is 0.601\n",
      "[[206  95]\n",
      " [139 147]]\n",
      "0.6013628620102215\n",
      "using loss squared_hinge,  penalty is None,  alpha is 5, the acc is 0.630\n",
      "[[262  39]\n",
      " [178 108]]\n",
      "0.6303236797274276\n",
      "using loss squared_hinge,  penalty is None,  alpha is 10, the acc is 0.622\n",
      "[[246  55]\n",
      " [167 119]]\n",
      "0.6218057921635435\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.001, the acc is 0.514\n",
      "[[301   0]\n",
      " [285   1]]\n",
      "0.514480408858603\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss perceptron,  penalty is l1,  alpha is 1, the acc is 0.625\n",
      "[[274  27]\n",
      " [193  93]]\n",
      "0.6252129471890971\n",
      "using loss perceptron,  penalty is l1,  alpha is 5, the acc is 0.629\n",
      "[[250  51]\n",
      " [167 119]]\n",
      "0.6286201022146508\n",
      "using loss perceptron,  penalty is l1,  alpha is 10, the acc is 0.627\n",
      "[[255  46]\n",
      " [173 113]]\n",
      "0.626916524701874\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.001, the acc is 0.618\n",
      "[[166 135]\n",
      " [ 89 197]]\n",
      "0.6183986371379898\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.1, the acc is 0.606\n",
      "[[147 154]\n",
      " [ 77 209]]\n",
      "0.606473594548552\n",
      "using loss perceptron,  penalty is l2,  alpha is 1, the acc is 0.603\n",
      "[[197 104]\n",
      " [129 157]]\n",
      "0.6030664395229983\n",
      "using loss perceptron,  penalty is l2,  alpha is 5, the acc is 0.608\n",
      "[[227  74]\n",
      " [156 130]]\n",
      "0.6081771720613288\n",
      "using loss perceptron,  penalty is l2,  alpha is 10, the acc is 0.606\n",
      "[[169 132]\n",
      " [ 99 187]]\n",
      "0.606473594548552\n",
      "using loss perceptron,  penalty is None,  alpha is 0.001, the acc is 0.630\n",
      "[[231  70]\n",
      " [147 139]]\n",
      "0.6303236797274276\n",
      "using loss perceptron,  penalty is None,  alpha is 0.1, the acc is 0.646\n",
      "[[133 168]\n",
      " [ 40 246]]\n",
      "0.645655877342419\n",
      "using loss perceptron,  penalty is None,  alpha is 1, the acc is 0.646\n",
      "[[133 168]\n",
      " [ 40 246]]\n",
      "0.645655877342419\n",
      "using loss perceptron,  penalty is None,  alpha is 5, the acc is 0.603\n",
      "[[224  77]\n",
      " [156 130]]\n",
      "0.6030664395229983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss perceptron,  penalty is None,  alpha is 10, the acc is 0.608\n",
      "[[226  75]\n",
      " [155 131]]\n",
      "0.6081771720613288\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.001, the acc is 0.501\n",
      "[[293   8]\n",
      " [285   1]]\n",
      "0.5008517887563884\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.1, the acc is 0.526\n",
      "[[ 23 278]\n",
      " [  0 286]]\n",
      "0.5264054514480409\n",
      "using loss squared_error,  penalty is l1,  alpha is 1, the acc is 0.455\n",
      "[[216  85]\n",
      " [235  51]]\n",
      "0.454855195911414\n",
      "using loss squared_error,  penalty is l1,  alpha is 5, the acc is 0.385\n",
      "[[ 99 202]\n",
      " [159 127]]\n",
      "0.3850085178875639\n",
      "using loss squared_error,  penalty is l1,  alpha is 10, the acc is 0.533\n",
      "[[254  47]\n",
      " [227  59]]\n",
      "0.5332197614991482\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.1, the acc is 0.508\n",
      "[[297   4]\n",
      " [285   1]]\n",
      "0.5076660988074957\n",
      "using loss squared_error,  penalty is l2,  alpha is 1, the acc is 0.417\n",
      "[[200 101]\n",
      " [241  45]]\n",
      "0.41737649063032367\n",
      "using loss squared_error,  penalty is l2,  alpha is 5, the acc is 0.620\n",
      "[[185 116]\n",
      " [107 179]]\n",
      "0.6201022146507666\n",
      "using loss squared_error,  penalty is l2,  alpha is 10, the acc is 0.455\n",
      "[[ 43 258]\n",
      " [ 62 224]]\n",
      "0.454855195911414\n",
      "using loss squared_error,  penalty is None,  alpha is 0.001, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_error,  penalty is None,  alpha is 0.1, the acc is 0.497\n",
      "[[284  17]\n",
      " [278   8]]\n",
      "0.49744463373083475\n",
      "using loss squared_error,  penalty is None,  alpha is 1, the acc is 0.460\n",
      "[[179 122]\n",
      " [195  91]]\n",
      "0.4599659284497445\n",
      "using loss squared_error,  penalty is None,  alpha is 5, the acc is 0.378\n",
      "[[ 90 211]\n",
      " [154 132]]\n",
      "0.3781942078364566\n",
      "using loss squared_error,  penalty is None,  alpha is 10, the acc is 0.528\n",
      "[[255  46]\n",
      " [231  55]]\n",
      "0.5281090289608177\n",
      "using loss huber,  penalty is l1,  alpha is 0.001, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss huber,  penalty is l1,  alpha is 0.1, the acc is 0.581\n",
      "[[299   2]\n",
      " [244  42]]\n",
      "0.5809199318568995\n",
      "using loss huber,  penalty is l1,  alpha is 1, the acc is 0.613\n",
      "[[171 130]\n",
      " [ 97 189]]\n",
      "0.6132879045996593\n",
      "using loss huber,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 0.001, the acc is 0.578\n",
      "[[283  18]\n",
      " [230  56]]\n",
      "0.5775127768313458\n",
      "using loss huber,  penalty is l2,  alpha is 0.1, the acc is 0.605\n",
      "[[248  53]\n",
      " [179 107]]\n",
      "0.6047700170357752\n",
      "using loss huber,  penalty is l2,  alpha is 1, the acc is 0.624\n",
      "[[275  26]\n",
      " [195  91]]\n",
      "0.6235093696763203\n",
      "using loss huber,  penalty is l2,  alpha is 5, the acc is 0.593\n",
      "[[189 112]\n",
      " [127 159]]\n",
      "0.5928449744463373\n",
      "using loss huber,  penalty is l2,  alpha is 10, the acc is 0.608\n",
      "[[165 136]\n",
      " [ 94 192]]\n",
      "0.6081771720613288\n",
      "using loss huber,  penalty is None,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is None,  alpha is 0.1, the acc is 0.608\n",
      "[[286  15]\n",
      " [215  71]]\n",
      "0.6081771720613288\n",
      "using loss huber,  penalty is None,  alpha is 1, the acc is 0.610\n",
      "[[278  23]\n",
      " [206  80]]\n",
      "0.6098807495741057\n",
      "using loss huber,  penalty is None,  alpha is 5, the acc is 0.615\n",
      "[[263  38]\n",
      " [188  98]]\n",
      "0.6149914821124361\n",
      "using loss huber,  penalty is None,  alpha is 10, the acc is 0.608\n",
      "[[280  21]\n",
      " [209  77]]\n",
      "0.6081771720613288\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.554\n",
      "[[299   2]\n",
      " [260  26]]\n",
      "0.5536626916524702\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.624\n",
      "[[285  16]\n",
      " [205  81]]\n",
      "0.6235093696763203\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.627\n",
      "[[253  48]\n",
      " [171 115]]\n",
      "0.626916524701874\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.368\n",
      "[[151 150]\n",
      " [221  65]]\n",
      "0.3679727427597956\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.622\n",
      "[[ 88 213]\n",
      " [  9 277]]\n",
      "0.6218057921635435\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.608\n",
      "[[293   8]\n",
      " [222  64]]\n",
      "0.6081771720613288\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.598\n",
      "[[296   5]\n",
      " [231  55]]\n",
      "0.5979557069846678\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.530\n",
      "[[247  54]\n",
      " [222  64]]\n",
      "0.5298126064735945\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.613\n",
      "[[288  13]\n",
      " [214  72]]\n",
      "0.6132879045996593\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.579\n",
      "[[164 137]\n",
      " [110 176]]\n",
      "0.5792163543441227\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.601\n",
      "[[277  24]\n",
      " [210  76]]\n",
      "0.6013628620102215\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.606\n",
      "[[281  20]\n",
      " [211  75]]\n",
      "0.606473594548552\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.501\n",
      "[[  9 292]\n",
      " [  1 285]]\n",
      "0.5008517887563884\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.557\n",
      "[[ 79 222]\n",
      " [ 38 248]]\n",
      "0.5570698466780238\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.562\n",
      "[[252  49]\n",
      " [208  78]]\n",
      "0.5621805792163543\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.385\n",
      "[[ 99 202]\n",
      " [159 127]]\n",
      "0.3850085178875639\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.509\n",
      "[[103 198]\n",
      " [ 90 196]]\n",
      "0.5093696763202725\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.499\n",
      "[[  7 294]\n",
      " [  0 286]]\n",
      "0.4991482112436116\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.402\n",
      "[[104 197]\n",
      " [154 132]]\n",
      "0.4020442930153322\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.496\n",
      "[[ 51 250]\n",
      " [ 46 240]]\n",
      "0.4957410562180579\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.513\n",
      "[[300   1]\n",
      " [285   1]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.504\n",
      "[[ 11 290]\n",
      " [  1 285]]\n",
      "0.504258943781942\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.405\n",
      "[[ 93 208]\n",
      " [141 145]]\n",
      "0.40545144804088584\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.424\n",
      "[[169 132]\n",
      " [206  80]]\n",
      "0.424190800681431\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.382\n",
      "[[107 194]\n",
      " [169 117]]\n",
      "0.38160136286201024\n"
     ]
    }
   ],
   "source": [
    "#SGDClassification\n",
    "X=dfAll.drop(columns = ['AUTHOR','CONTENT','CLASS'])\n",
    "y=dfAll['CLASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "losses = ['hinge', 'log_loss', 'log', 'modified_huber', \n",
    "          'squared_hinge', 'perceptron', 'squared_error',\n",
    "          'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalties = ['l1','l2',None]\n",
    "alphas = [0.001, .1, 1, 5, 10]\n",
    "for _loss in losses: \n",
    "    for _penalty in penalties:\n",
    "        for _alpha in alphas:\n",
    "            model = SGDClassifier(loss=_loss, penalty=_penalty, alpha=_alpha, random_state=10)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            print(\"using loss {}\".format(_loss) + \",  penalty is {}\".format(_penalty) +\n",
    "                  \",  alpha is {}\".format(_alpha) + \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "            print(confusion_matrix(y_test, pred))\n",
    "            print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63bad237-cc6a-4485-8dd0-816e4e0cc368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[205  96]\n",
      " [ 94 192]]\n",
      "0.676320272572402\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier:\n",
    "model = DecisionTreeClassifier(random_state = 10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c9c74e7-d616-4d6d-8b10-da7ffb3acf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[205  96]\n",
      " [ 94 192]]\n",
      "0.676320272572402\n"
     ]
    }
   ],
   "source": [
    "#DecisionTree had the highest acc\n",
    "model = DecisionTreeClassifier(random_state = 10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e9b9cf-7b78-4fcf-a0bf-880522d4163c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code taken from https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "#Generating ROC and AUC for Linear & C=1\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC / AUC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e55e75-108d-4455-b205-3da68ee68a48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) C_LEN                                     0.581792\n",
      " 2) A_LEN                                     0.207746\n",
      " 3) C_SPEC                                    0.200967\n",
      " 4) A_SPEC                                    0.009494\n"
     ]
    }
   ],
   "source": [
    "#Testing importance of each attribute\n",
    "feat_labels = X.columns[:]\n",
    "\n",
    "forest = RandomForestClassifier(random_state = 10)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    if f < 15:\n",
    "        print(\"%2d) %-*s %f\" % (f + 1, 41, feat_labels[indices[f]], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0105eb-fad5-4ed5-93f0-fa9488496981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It appears that A_SPEC is insignifant. Let's try running the model without it\n",
    "X=dfAll.drop(columns = ['AUTHOR','CONTENT','CLASS','A_SPEC'])\n",
    "y=dfAll['CLASS']\n",
    "\n",
    "#Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a149e35-7f4c-4af8-8a4b-b9668f394231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using kernel linear, C value is 0.01, the acc is 0.639\n",
      "[[249  52]\n",
      " [160 126]]\n",
      "using kernel linear, C value is 0.1, the acc is 0.639\n",
      "[[249  52]\n",
      " [160 126]]\n",
      "using kernel linear, C value is 0.5, the acc is 0.639\n",
      "[[249  52]\n",
      " [160 126]]\n",
      "using kernel linear, C value is 1, the acc is 0.639\n",
      "[[249  52]\n",
      " [160 126]]\n",
      "using kernel linear, C value is 5, the acc is 0.639\n",
      "[[249  52]\n",
      " [160 126]]\n",
      "using kernel linear, C value is 10, the acc is 0.639\n",
      "[[249  52]\n",
      " [160 126]]\n",
      "using kernel rbf, C value is 0.01, the acc is 0.627\n",
      "[[253  48]\n",
      " [171 115]]\n",
      "using kernel rbf, C value is 0.1, the acc is 0.612\n",
      "[[217  84]\n",
      " [144 142]]\n",
      "using kernel rbf, C value is 0.5, the acc is 0.618\n",
      "[[187 114]\n",
      " [110 176]]\n",
      "using kernel rbf, C value is 1, the acc is 0.620\n",
      "[[183 118]\n",
      " [105 181]]\n",
      "using kernel rbf, C value is 5, the acc is 0.612\n",
      "[[166 135]\n",
      " [ 93 193]]\n",
      "using kernel rbf, C value is 10, the acc is 0.605\n",
      "[[154 147]\n",
      " [ 85 201]]\n",
      "using kernel poly, C value is 0.01, the acc is 0.596\n",
      "[[296   5]\n",
      " [232  54]]\n",
      "using kernel poly, C value is 0.1, the acc is 0.603\n",
      "[[294   7]\n",
      " [226  60]]\n",
      "using kernel poly, C value is 0.5, the acc is 0.612\n",
      "[[292   9]\n",
      " [219  67]]\n",
      "using kernel poly, C value is 1, the acc is 0.624\n",
      "[[291  10]\n",
      " [211  75]]\n",
      "using kernel poly, C value is 5, the acc is 0.630\n",
      "[[288  13]\n",
      " [204  82]]\n",
      "using kernel poly, C value is 10, the acc is 0.625\n",
      "[[287  14]\n",
      " [206  80]]\n"
     ]
    }
   ],
   "source": [
    "#SVC: Testing several kernal types and C values\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.01, 0.1, 0.5, 1, 5, 10]\n",
    "for kernel in kernels: \n",
    "    for c_val in C_values: \n",
    "        model = SVC(kernel=kernel, C=c_val, probability = True, random_state=10)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        print(\"using kernel {}\".format(kernel) + \", C value is {}\".format(c_val) +\n",
    "              \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "        print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4279bc9-4412-4d56-b6ce-0499d89eda7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss hinge,  penalty is l1,  alpha is 0.001, the acc is 0.514\n",
      "[[301   0]\n",
      " [285   1]]\n",
      "0.514480408858603\n",
      "using loss hinge,  penalty is l1,  alpha is 0.1, the acc is 0.652\n",
      "[[122 179]\n",
      " [ 25 261]]\n",
      "0.6524701873935264\n",
      "using loss hinge,  penalty is l1,  alpha is 1, the acc is 0.620\n",
      "[[193 108]\n",
      " [115 171]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is l1,  alpha is 5, the acc is 0.620\n",
      "[[184 117]\n",
      " [106 180]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is l1,  alpha is 10, the acc is 0.613\n",
      "[[217  84]\n",
      " [143 143]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is l2,  alpha is 0.001, the acc is 0.605\n",
      "[[118 183]\n",
      " [ 49 237]]\n",
      "0.6047700170357752\n",
      "using loss hinge,  penalty is l2,  alpha is 0.1, the acc is 0.600\n",
      "[[200 101]\n",
      " [134 152]]\n",
      "0.5996592844974447\n",
      "using loss hinge,  penalty is l2,  alpha is 1, the acc is 0.613\n",
      "[[219  82]\n",
      " [145 141]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is l2,  alpha is 5, the acc is 0.606\n",
      "[[197 104]\n",
      " [127 159]]\n",
      "0.606473594548552\n",
      "using loss hinge,  penalty is l2,  alpha is 10, the acc is 0.612\n",
      "[[224  77]\n",
      " [151 135]]\n",
      "0.6115843270868825\n",
      "using loss hinge,  penalty is None,  alpha is 0.001, the acc is 0.641\n",
      "[[290  11]\n",
      " [200  86]]\n",
      "0.6405451448040886\n",
      "using loss hinge,  penalty is None,  alpha is 0.1, the acc is 0.613\n",
      "[[181 120]\n",
      " [107 179]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is None,  alpha is 1, the acc is 0.610\n",
      "[[208  93]\n",
      " [136 150]]\n",
      "0.6098807495741057\n",
      "using loss hinge,  penalty is None,  alpha is 5, the acc is 0.603\n",
      "[[223  78]\n",
      " [155 131]]\n",
      "0.6030664395229983\n",
      "using loss hinge,  penalty is None,  alpha is 10, the acc is 0.608\n",
      "[[226  75]\n",
      " [155 131]]\n",
      "0.6081771720613288\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.001, the acc is 0.634\n",
      "[[278  23]\n",
      " [192  94]]\n",
      "0.6337308347529813\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.1, the acc is 0.625\n",
      "[[196 105]\n",
      " [115 171]]\n",
      "0.6252129471890971\n",
      "using loss log_loss,  penalty is l1,  alpha is 1, the acc is 0.624\n",
      "[[280  21]\n",
      " [200  86]]\n",
      "0.6235093696763203\n",
      "using loss log_loss,  penalty is l1,  alpha is 5, the acc is 0.613\n",
      "[[166 135]\n",
      " [ 92 194]]\n",
      "0.6132879045996593\n",
      "using loss log_loss,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.001, the acc is 0.622\n",
      "[[158 143]\n",
      " [ 79 207]]\n",
      "0.6218057921635435\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.1, the acc is 0.600\n",
      "[[181 120]\n",
      " [115 171]]\n",
      "0.5996592844974447\n",
      "using loss log_loss,  penalty is l2,  alpha is 1, the acc is 0.639\n",
      "[[280  21]\n",
      " [191  95]]\n",
      "0.6388415672913118\n",
      "using loss log_loss,  penalty is l2,  alpha is 5, the acc is 0.601\n",
      "[[177 124]\n",
      " [110 176]]\n",
      "0.6013628620102215\n",
      "using loss log_loss,  penalty is l2,  alpha is 10, the acc is 0.603\n",
      "[[182 119]\n",
      " [114 172]]\n",
      "0.6030664395229983\n",
      "using loss log_loss,  penalty is None,  alpha is 0.001, the acc is 0.526\n",
      "[[296   5]\n",
      " [273  13]]\n",
      "0.5264054514480409\n",
      "using loss log_loss,  penalty is None,  alpha is 0.1, the acc is 0.618\n",
      "[[186 115]\n",
      " [109 177]]\n",
      "0.6183986371379898\n",
      "using loss log_loss,  penalty is None,  alpha is 1, the acc is 0.634\n",
      "[[279  22]\n",
      " [193  93]]\n",
      "0.6337308347529813\n",
      "using loss log_loss,  penalty is None,  alpha is 5, the acc is 0.612\n",
      "[[220  81]\n",
      " [147 139]]\n",
      "0.6115843270868825\n",
      "using loss log_loss,  penalty is None,  alpha is 10, the acc is 0.603\n",
      "[[230  71]\n",
      " [162 124]]\n",
      "0.6030664395229983\n",
      "using loss log,  penalty is l1,  alpha is 0.001, the acc is 0.634\n",
      "[[278  23]\n",
      " [192  94]]\n",
      "0.6337308347529813\n",
      "using loss log,  penalty is l1,  alpha is 0.1, the acc is 0.625\n",
      "[[196 105]\n",
      " [115 171]]\n",
      "0.6252129471890971\n",
      "using loss log,  penalty is l1,  alpha is 1, the acc is 0.624\n",
      "[[280  21]\n",
      " [200  86]]\n",
      "0.6235093696763203\n",
      "using loss log,  penalty is l1,  alpha is 5, the acc is 0.613\n",
      "[[166 135]\n",
      " [ 92 194]]\n",
      "0.6132879045996593\n",
      "using loss log,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l2,  alpha is 0.001, the acc is 0.622\n",
      "[[158 143]\n",
      " [ 79 207]]\n",
      "0.6218057921635435\n",
      "using loss log,  penalty is l2,  alpha is 0.1, the acc is 0.600\n",
      "[[181 120]\n",
      " [115 171]]\n",
      "0.5996592844974447\n",
      "using loss log,  penalty is l2,  alpha is 1, the acc is 0.639\n",
      "[[280  21]\n",
      " [191  95]]\n",
      "0.6388415672913118\n",
      "using loss log,  penalty is l2,  alpha is 5, the acc is 0.601\n",
      "[[177 124]\n",
      " [110 176]]\n",
      "0.6013628620102215\n",
      "using loss log,  penalty is l2,  alpha is 10, the acc is 0.603\n",
      "[[182 119]\n",
      " [114 172]]\n",
      "0.6030664395229983\n",
      "using loss log,  penalty is None,  alpha is 0.001, the acc is 0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[296   5]\n",
      " [273  13]]\n",
      "0.5264054514480409\n",
      "using loss log,  penalty is None,  alpha is 0.1, the acc is 0.618\n",
      "[[186 115]\n",
      " [109 177]]\n",
      "0.6183986371379898\n",
      "using loss log,  penalty is None,  alpha is 1, the acc is 0.634\n",
      "[[279  22]\n",
      " [193  93]]\n",
      "0.6337308347529813\n",
      "using loss log,  penalty is None,  alpha is 5, the acc is 0.612\n",
      "[[220  81]\n",
      " [147 139]]\n",
      "0.6115843270868825\n",
      "using loss log,  penalty is None,  alpha is 10, the acc is 0.603\n",
      "[[230  71]\n",
      " [162 124]]\n",
      "0.6030664395229983\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.001, the acc is 0.661\n",
      "[[126 175]\n",
      " [ 24 262]]\n",
      "0.6609880749574105\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.1, the acc is 0.610\n",
      "[[214  87]\n",
      " [142 144]]\n",
      "0.6098807495741057\n",
      "using loss modified_huber,  penalty is l1,  alpha is 1, the acc is 0.606\n",
      "[[217  84]\n",
      " [147 139]]\n",
      "0.606473594548552\n",
      "using loss modified_huber,  penalty is l1,  alpha is 5, the acc is 0.627\n",
      "[[248  53]\n",
      " [166 120]]\n",
      "0.626916524701874\n",
      "using loss modified_huber,  penalty is l1,  alpha is 10, the acc is 0.625\n",
      "[[283  18]\n",
      " [202  84]]\n",
      "0.6252129471890971\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.001, the acc is 0.605\n",
      "[[118 183]\n",
      " [ 49 237]]\n",
      "0.6047700170357752\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.1, the acc is 0.596\n",
      "[[161 140]\n",
      " [ 97 189]]\n",
      "0.596252129471891\n",
      "using loss modified_huber,  penalty is l2,  alpha is 1, the acc is 0.595\n",
      "[[180 121]\n",
      " [117 169]]\n",
      "0.5945485519591142\n",
      "using loss modified_huber,  penalty is l2,  alpha is 5, the acc is 0.634\n",
      "[[227  74]\n",
      " [141 145]]\n",
      "0.6337308347529813\n",
      "using loss modified_huber,  penalty is l2,  alpha is 10, the acc is 0.639\n",
      "[[268  33]\n",
      " [179 107]]\n",
      "0.6388415672913118\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.001, the acc is 0.647\n",
      "[[117 184]\n",
      " [ 23 263]]\n",
      "0.6473594548551959\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.1, the acc is 0.613\n",
      "[[178 123]\n",
      " [104 182]]\n",
      "0.6132879045996593\n",
      "using loss modified_huber,  penalty is None,  alpha is 1, the acc is 0.625\n",
      "[[195 106]\n",
      " [114 172]]\n",
      "0.6252129471890971\n",
      "using loss modified_huber,  penalty is None,  alpha is 5, the acc is 0.622\n",
      "[[232  69]\n",
      " [153 133]]\n",
      "0.6218057921635435\n",
      "using loss modified_huber,  penalty is None,  alpha is 10, the acc is 0.612\n",
      "[[229  72]\n",
      " [156 130]]\n",
      "0.6115843270868825\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.001, the acc is 0.612\n",
      "[[174 127]\n",
      " [101 185]]\n",
      "0.6115843270868825\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.1, the acc is 0.608\n",
      "[[158 143]\n",
      " [ 87 199]]\n",
      "0.6081771720613288\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 1, the acc is 0.601\n",
      "[[206  95]\n",
      " [139 147]]\n",
      "0.6013628620102215\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 5, the acc is 0.634\n",
      "[[263  38]\n",
      " [177 109]]\n",
      "0.6337308347529813\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 10, the acc is 0.629\n",
      "[[250  51]\n",
      " [167 119]]\n",
      "0.6286201022146508\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.001, the acc is 0.612\n",
      "[[121 180]\n",
      " [ 48 238]]\n",
      "0.6115843270868825\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.1, the acc is 0.601\n",
      "[[143 158]\n",
      " [ 76 210]]\n",
      "0.6013628620102215\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 1, the acc is 0.605\n",
      "[[205  96]\n",
      " [136 150]]\n",
      "0.6047700170357752\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 5, the acc is 0.634\n",
      "[[256  45]\n",
      " [170 116]]\n",
      "0.6337308347529813\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 10, the acc is 0.627\n",
      "[[253  48]\n",
      " [171 115]]\n",
      "0.626916524701874\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.001, the acc is 0.615\n",
      "[[177 124]\n",
      " [102 184]]\n",
      "0.6149914821124361\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.1, the acc is 0.625\n",
      "[[149 152]\n",
      " [ 68 218]]\n",
      "0.6252129471890971\n",
      "using loss squared_hinge,  penalty is None,  alpha is 1, the acc is 0.601\n",
      "[[206  95]\n",
      " [139 147]]\n",
      "0.6013628620102215\n",
      "using loss squared_hinge,  penalty is None,  alpha is 5, the acc is 0.630\n",
      "[[262  39]\n",
      " [178 108]]\n",
      "0.6303236797274276\n",
      "using loss squared_hinge,  penalty is None,  alpha is 10, the acc is 0.622\n",
      "[[246  55]\n",
      " [167 119]]\n",
      "0.6218057921635435\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.001, the acc is 0.514\n",
      "[[301   0]\n",
      " [285   1]]\n",
      "0.514480408858603\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss perceptron,  penalty is l1,  alpha is 1, the acc is 0.625\n",
      "[[274  27]\n",
      " [193  93]]\n",
      "0.6252129471890971\n",
      "using loss perceptron,  penalty is l1,  alpha is 5, the acc is 0.629\n",
      "[[250  51]\n",
      " [167 119]]\n",
      "0.6286201022146508\n",
      "using loss perceptron,  penalty is l1,  alpha is 10, the acc is 0.627\n",
      "[[255  46]\n",
      " [173 113]]\n",
      "0.626916524701874\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.001, the acc is 0.618\n",
      "[[166 135]\n",
      " [ 89 197]]\n",
      "0.6183986371379898\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.1, the acc is 0.606\n",
      "[[147 154]\n",
      " [ 77 209]]\n",
      "0.606473594548552\n",
      "using loss perceptron,  penalty is l2,  alpha is 1, the acc is 0.603\n",
      "[[197 104]\n",
      " [129 157]]\n",
      "0.6030664395229983\n",
      "using loss perceptron,  penalty is l2,  alpha is 5, the acc is 0.608\n",
      "[[227  74]\n",
      " [156 130]]\n",
      "0.6081771720613288\n",
      "using loss perceptron,  penalty is l2,  alpha is 10, the acc is 0.606\n",
      "[[169 132]\n",
      " [ 99 187]]\n",
      "0.606473594548552\n",
      "using loss perceptron,  penalty is None,  alpha is 0.001, the acc is 0.630\n",
      "[[231  70]\n",
      " [147 139]]\n",
      "0.6303236797274276\n",
      "using loss perceptron,  penalty is None,  alpha is 0.1, the acc is 0.646\n",
      "[[133 168]\n",
      " [ 40 246]]\n",
      "0.645655877342419\n",
      "using loss perceptron,  penalty is None,  alpha is 1, the acc is 0.646\n",
      "[[133 168]\n",
      " [ 40 246]]\n",
      "0.645655877342419\n",
      "using loss perceptron,  penalty is None,  alpha is 5, the acc is 0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[224  77]\n",
      " [156 130]]\n",
      "0.6030664395229983\n",
      "using loss perceptron,  penalty is None,  alpha is 10, the acc is 0.608\n",
      "[[226  75]\n",
      " [155 131]]\n",
      "0.6081771720613288\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.001, the acc is 0.501\n",
      "[[293   8]\n",
      " [285   1]]\n",
      "0.5008517887563884\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.1, the acc is 0.526\n",
      "[[ 23 278]\n",
      " [  0 286]]\n",
      "0.5264054514480409\n",
      "using loss squared_error,  penalty is l1,  alpha is 1, the acc is 0.455\n",
      "[[216  85]\n",
      " [235  51]]\n",
      "0.454855195911414\n",
      "using loss squared_error,  penalty is l1,  alpha is 5, the acc is 0.385\n",
      "[[ 99 202]\n",
      " [159 127]]\n",
      "0.3850085178875639\n",
      "using loss squared_error,  penalty is l1,  alpha is 10, the acc is 0.533\n",
      "[[254  47]\n",
      " [227  59]]\n",
      "0.5332197614991482\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.1, the acc is 0.508\n",
      "[[297   4]\n",
      " [285   1]]\n",
      "0.5076660988074957\n",
      "using loss squared_error,  penalty is l2,  alpha is 1, the acc is 0.417\n",
      "[[200 101]\n",
      " [241  45]]\n",
      "0.41737649063032367\n",
      "using loss squared_error,  penalty is l2,  alpha is 5, the acc is 0.620\n",
      "[[185 116]\n",
      " [107 179]]\n",
      "0.6201022146507666\n",
      "using loss squared_error,  penalty is l2,  alpha is 10, the acc is 0.455\n",
      "[[ 43 258]\n",
      " [ 62 224]]\n",
      "0.454855195911414\n",
      "using loss squared_error,  penalty is None,  alpha is 0.001, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_error,  penalty is None,  alpha is 0.1, the acc is 0.497\n",
      "[[284  17]\n",
      " [278   8]]\n",
      "0.49744463373083475\n",
      "using loss squared_error,  penalty is None,  alpha is 1, the acc is 0.460\n",
      "[[179 122]\n",
      " [195  91]]\n",
      "0.4599659284497445\n",
      "using loss squared_error,  penalty is None,  alpha is 5, the acc is 0.378\n",
      "[[ 90 211]\n",
      " [154 132]]\n",
      "0.3781942078364566\n",
      "using loss squared_error,  penalty is None,  alpha is 10, the acc is 0.528\n",
      "[[255  46]\n",
      " [231  55]]\n",
      "0.5281090289608177\n",
      "using loss huber,  penalty is l1,  alpha is 0.001, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss huber,  penalty is l1,  alpha is 0.1, the acc is 0.581\n",
      "[[299   2]\n",
      " [244  42]]\n",
      "0.5809199318568995\n",
      "using loss huber,  penalty is l1,  alpha is 1, the acc is 0.613\n",
      "[[171 130]\n",
      " [ 97 189]]\n",
      "0.6132879045996593\n",
      "using loss huber,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 0.001, the acc is 0.578\n",
      "[[283  18]\n",
      " [230  56]]\n",
      "0.5775127768313458\n",
      "using loss huber,  penalty is l2,  alpha is 0.1, the acc is 0.605\n",
      "[[248  53]\n",
      " [179 107]]\n",
      "0.6047700170357752\n",
      "using loss huber,  penalty is l2,  alpha is 1, the acc is 0.624\n",
      "[[275  26]\n",
      " [195  91]]\n",
      "0.6235093696763203\n",
      "using loss huber,  penalty is l2,  alpha is 5, the acc is 0.593\n",
      "[[189 112]\n",
      " [127 159]]\n",
      "0.5928449744463373\n",
      "using loss huber,  penalty is l2,  alpha is 10, the acc is 0.608\n",
      "[[165 136]\n",
      " [ 94 192]]\n",
      "0.6081771720613288\n",
      "using loss huber,  penalty is None,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is None,  alpha is 0.1, the acc is 0.608\n",
      "[[286  15]\n",
      " [215  71]]\n",
      "0.6081771720613288\n",
      "using loss huber,  penalty is None,  alpha is 1, the acc is 0.610\n",
      "[[278  23]\n",
      " [206  80]]\n",
      "0.6098807495741057\n",
      "using loss huber,  penalty is None,  alpha is 5, the acc is 0.615\n",
      "[[263  38]\n",
      " [188  98]]\n",
      "0.6149914821124361\n",
      "using loss huber,  penalty is None,  alpha is 10, the acc is 0.608\n",
      "[[280  21]\n",
      " [209  77]]\n",
      "0.6081771720613288\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.554\n",
      "[[299   2]\n",
      " [260  26]]\n",
      "0.5536626916524702\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.624\n",
      "[[285  16]\n",
      " [205  81]]\n",
      "0.6235093696763203\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.627\n",
      "[[253  48]\n",
      " [171 115]]\n",
      "0.626916524701874\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.368\n",
      "[[151 150]\n",
      " [221  65]]\n",
      "0.3679727427597956\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.622\n",
      "[[ 88 213]\n",
      " [  9 277]]\n",
      "0.6218057921635435\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.608\n",
      "[[293   8]\n",
      " [222  64]]\n",
      "0.6081771720613288\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.598\n",
      "[[296   5]\n",
      " [231  55]]\n",
      "0.5979557069846678\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.530\n",
      "[[247  54]\n",
      " [222  64]]\n",
      "0.5298126064735945\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.613\n",
      "[[288  13]\n",
      " [214  72]]\n",
      "0.6132879045996593\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.579\n",
      "[[164 137]\n",
      " [110 176]]\n",
      "0.5792163543441227\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.601\n",
      "[[277  24]\n",
      " [210  76]]\n",
      "0.6013628620102215\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.606\n",
      "[[281  20]\n",
      " [211  75]]\n",
      "0.606473594548552\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.501\n",
      "[[  9 292]\n",
      " [  1 285]]\n",
      "0.5008517887563884\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.557\n",
      "[[ 79 222]\n",
      " [ 38 248]]\n",
      "0.5570698466780238\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.562\n",
      "[[252  49]\n",
      " [208  78]]\n",
      "0.5621805792163543\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.385\n",
      "[[ 99 202]\n",
      " [159 127]]\n",
      "0.3850085178875639\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.509\n",
      "[[103 198]\n",
      " [ 90 196]]\n",
      "0.5093696763202725\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.499\n",
      "[[  7 294]\n",
      " [  0 286]]\n",
      "0.4991482112436116\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.402\n",
      "[[104 197]\n",
      " [154 132]]\n",
      "0.4020442930153322\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.496\n",
      "[[ 51 250]\n",
      " [ 46 240]]\n",
      "0.4957410562180579\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.513\n",
      "[[300   1]\n",
      " [285   1]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.504\n",
      "[[ 11 290]\n",
      " [  1 285]]\n",
      "0.504258943781942\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.405\n",
      "[[ 93 208]\n",
      " [141 145]]\n",
      "0.40545144804088584\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.424\n",
      "[[169 132]\n",
      " [206  80]]\n",
      "0.424190800681431\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.382\n",
      "[[107 194]\n",
      " [169 117]]\n",
      "0.38160136286201024\n"
     ]
    }
   ],
   "source": [
    "#SGDClassification\n",
    "X=dfAll.drop(columns = ['AUTHOR','CONTENT','CLASS'])\n",
    "y=dfAll['CLASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "losses = ['hinge', 'log_loss', 'log', 'modified_huber', \n",
    "          'squared_hinge', 'perceptron', 'squared_error',\n",
    "          'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalties = ['l1','l2',None]\n",
    "alphas = [0.001, .1, 1, 5, 10]\n",
    "for _loss in losses: \n",
    "    for _penalty in penalties:\n",
    "        for _alpha in alphas:\n",
    "            model = SGDClassifier(loss=_loss, penalty=_penalty, alpha=_alpha, random_state=10)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            print(\"using loss {}\".format(_loss) + \",  penalty is {}\".format(_penalty) +\n",
    "                  \",  alpha is {}\".format(_alpha) + \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "            print(confusion_matrix(y_test, pred))\n",
    "            print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b35b7e42-c9c4-4fa1-b000-16b905c3acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[205  96]\n",
      " [ 94 192]]\n",
      "0.676320272572402\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier:\n",
    "model = DecisionTreeClassifier(random_state = 10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "216a3a02-cd30-4106-9b24-8a88ec9bc042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[205  96]\n",
      " [ 94 192]]\n",
      "0.676320272572402\n"
     ]
    }
   ],
   "source": [
    "#DecisionTree had highest acc\n",
    "model = DecisionTreeClassifier(random_state = 10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84f83630-a9ba-4dbd-9edb-1d3c964a166c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code taken from https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "#Generating ROC and AUC for Linear & C=1\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC / AUC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23c2bd-e226-46b4-8aa6-e94a0ce19283",
   "metadata": {},
   "source": [
    "Part 2 - Using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "152876a3-0215-4e36-9a6e-7fc550508c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eddie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eddie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3d4e6605-02bb-481b-9ce7-97eb7b92a8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i turned it on mute as soon is i came on i jus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm only checking the views﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i think about 100 millions of the views come f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just checking the views﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I dont even watch it anymore i just come here ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>951 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               CONTENT  CLASS\n",
       "0    i turned it on mute as soon is i came on i jus...      0\n",
       "1                         I'm only checking the views﻿      0\n",
       "2    i think about 100 millions of the views come f...      0\n",
       "3                             just checking the views﻿      0\n",
       "4    I dont even watch it anymore i just come here ...      0\n",
       "..                                                 ...    ...\n",
       "946  I love this song because we sing it at Camp al...      0\n",
       "947  I love this song for two reasons: 1.it is abou...      0\n",
       "948                                                wow      0\n",
       "949                            Shakira u are so wiredo      0\n",
       "950                         Shakira is the best dancer      0\n",
       "\n",
       "[951 rows x 2 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSpam = dfAll.drop(columns = ['AUTHOR','C_LEN','A_LEN','C_SPEC','A_SPEC'])\n",
    "dfSpam = dfSpam[dfSpam['CLASS']==1]\n",
    "dfSpam = dfSpam.reset_index().drop('index', axis=1)\n",
    "\n",
    "dfHam = dfAll.drop(columns = ['AUTHOR','C_LEN','A_LEN','C_SPEC','A_SPEC'])\n",
    "dfHam= dfHam[dfHam['CLASS']==0]\n",
    "dfHam = dfHam.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0140d81e-2cca-4b98-a9d0-3ee342403673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam tokens before no stop: 24655 \n",
      "Spam after no stop removed: 14855\n",
      "Ham tokens before no stop: 8879 \n",
      "Ham after no stop removed: 5064\n"
     ]
    }
   ],
   "source": [
    "tokens_spam = []\n",
    "for comment in dfSpam['CONTENT']:\n",
    "    for i in (tokenizer.tokenize(comment)):\n",
    "        tokens_spam.append(i.lower())\n",
    "\n",
    "tokens_spam_nostop = [i for i in tokens_spam if i not in stopwords]\n",
    "print(\"Spam tokens before no stop:\",len(tokens_spam), \"\\nSpam after no stop removed:\", len(tokens_spam_nostop))\n",
    "\n",
    "\n",
    "tokens_ham = []\n",
    "for comment in dfHam['CONTENT']:\n",
    "    for i in (tokenizer.tokenize(comment)):\n",
    "        tokens_ham.append(i.lower())\n",
    "\n",
    "tokens_ham_nostop = [i for i in tokens_ham if i not in stopwords]\n",
    "print(\"Ham tokens before no stop:\", len(tokens_ham), \"\\nHam after no stop removed:\", len(tokens_ham_nostop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6172f89b-453d-4746-b446-73da4caea9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tokens in Spam:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('check', 559),\n",
       " ('com', 296),\n",
       " ('please', 246),\n",
       " ('youtube', 235),\n",
       " ('subscribe', 229),\n",
       " ('video', 229),\n",
       " ('39', 210),\n",
       " ('channel', 197),\n",
       " ('br', 195),\n",
       " ('like', 160)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common tokens in Spam:\")\n",
    "Counter(tokens_spam_nostop).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4ffb467e-c920-4098-be5e-6441194d10d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tokens in Ham:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('song', 224),\n",
       " ('love', 145),\n",
       " ('like', 90),\n",
       " ('views', 87),\n",
       " ('video', 84),\n",
       " ('br', 64),\n",
       " ('best', 57),\n",
       " ('katy', 56),\n",
       " ('2', 55),\n",
       " ('billion', 51)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common tokens in Ham:\")\n",
    "Counter(tokens_ham_nostop).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "810d8a68-a8c7-4652-acb3-f506bf47610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNew = dfAll.drop(columns=['C_LEN','A_LEN','C_SPEC','A_SPEC'])\n",
    "\n",
    "check = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'check' in i.lower():\n",
    "        check.append(1)\n",
    "    else:\n",
    "        check.append(0)\n",
    "\n",
    "com = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'com' in i.lower():\n",
    "        com.append(1)\n",
    "    else:\n",
    "        com.append(0)    \n",
    "        \n",
    "please = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'please' in i.lower():\n",
    "        please.append(1)\n",
    "    else:\n",
    "        please.append(0)\n",
    "        \n",
    "youtube = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'youtube' in i.lower():\n",
    "        youtube.append(1)\n",
    "    else:\n",
    "        youtube.append(0)\n",
    "        \n",
    "subscribe = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'subscribe' in i.lower():\n",
    "        subscribe.append(1)\n",
    "    else:\n",
    "        subscribe.append(0)\n",
    "        \n",
    "channel = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'channel' in i.lower():\n",
    "        channel.append(1)\n",
    "    else:\n",
    "        channel.append(0)\n",
    "        \n",
    "song = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'song' in i.lower():\n",
    "        song.append(1)\n",
    "    else:\n",
    "        song.append(0)\n",
    "        \n",
    "love = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'love' in i.lower():\n",
    "        love.append(1)\n",
    "    else:\n",
    "        love.append(0)\n",
    "        \n",
    "views = []\n",
    "for i in dfNew['CONTENT']:\n",
    "    if 'views' in i.lower():\n",
    "        views.append(1)\n",
    "    else:\n",
    "        views.append(0)\n",
    "    \n",
    "\n",
    "dfNew['check'] = check\n",
    "dfNew['com'] = com\n",
    "dfNew['please'] = please\n",
    "dfNew['youtube'] = youtube\n",
    "dfNew['subscribe'] = subscribe\n",
    "dfNew['channel'] = channel\n",
    "dfNew['song'] = song\n",
    "dfNew['love'] = love\n",
    "dfNew['views'] = views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "36a18f8a-7fca-4585-85a4-58d20cf24fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing variables for training\n",
    "X=dfNew.drop(columns = ['AUTHOR','CONTENT','CLASS'])\n",
    "y=dfNew['CLASS']\n",
    "\n",
    "#Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "23692c42-f678-463f-9923-1cd1e2b93d09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using kernel linear, C value is 0.01, the acc is 0.884\n",
      "[[275  26]\n",
      " [ 42 244]]\n",
      "using kernel linear, C value is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 0.5, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "using kernel linear, C value is 1, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "using kernel linear, C value is 5, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "using kernel linear, C value is 10, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "using kernel rbf, C value is 0.01, the acc is 0.891\n",
      "[[266  35]\n",
      " [ 29 257]]\n",
      "using kernel rbf, C value is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 0.5, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 1, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 5, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 10, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 0.01, the acc is 0.826\n",
      "[[282  19]\n",
      " [ 83 203]]\n",
      "using kernel poly, C value is 0.1, the acc is 0.905\n",
      "[[280  21]\n",
      " [ 35 251]]\n",
      "using kernel poly, C value is 0.5, the acc is 0.913\n",
      "[[281  20]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 1, the acc is 0.913\n",
      "[[281  20]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 5, the acc is 0.913\n",
      "[[281  20]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 10, the acc is 0.913\n",
      "[[281  20]\n",
      " [ 31 255]]\n"
     ]
    }
   ],
   "source": [
    "#SVC: Testing several kernal types and C values\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.01, 0.1, 0.5, 1, 5, 10]\n",
    "for kernel in kernels: \n",
    "    for c_val in C_values: \n",
    "        model = SVC(kernel=kernel, C=c_val, probability = True, random_state=10)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        print(\"using kernel {}\".format(kernel) + \", C value is {}\".format(c_val) +\n",
    "              \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "        print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5abcdab3-97c3-4d60-8277-e0e0b1260cc1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss hinge,  penalty is l1,  alpha is 0.001, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "0.9080068143100511\n",
      "using loss hinge,  penalty is l1,  alpha is 0.1, the acc is 0.726\n",
      "[[293   8]\n",
      " [153 133]]\n",
      "0.7257240204429302\n",
      "using loss hinge,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is l2,  alpha is 0.001, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "0.9080068143100511\n",
      "using loss hinge,  penalty is l2,  alpha is 0.1, the acc is 0.842\n",
      "[[279  22]\n",
      " [ 71 215]]\n",
      "0.8415672913117547\n",
      "using loss hinge,  penalty is l2,  alpha is 1, the acc is 0.528\n",
      "[[301   0]\n",
      " [277   9]]\n",
      "0.5281090289608177\n",
      "using loss hinge,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is None,  alpha is 0.001, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "0.9114139693356048\n",
      "using loss hinge,  penalty is None,  alpha is 0.1, the acc is 0.899\n",
      "[[274  27]\n",
      " [ 32 254]]\n",
      "0.899488926746167\n",
      "using loss hinge,  penalty is None,  alpha is 1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss hinge,  penalty is None,  alpha is 5, the acc is 0.722\n",
      "[[297   4]\n",
      " [159 127]]\n",
      "0.7223168654173765\n",
      "using loss hinge,  penalty is None,  alpha is 10, the acc is 0.525\n",
      "[[301   0]\n",
      " [279   7]]\n",
      "0.524701873935264\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss log_loss,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.1, the acc is 0.898\n",
      "[[272  29]\n",
      " [ 31 255]]\n",
      "0.8977853492333902\n",
      "using loss log_loss,  penalty is l2,  alpha is 1, the acc is 0.802\n",
      "[[291  10]\n",
      " [106 180]]\n",
      "0.8023850085178875\n",
      "using loss log_loss,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is None,  alpha is 0.001, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "0.9114139693356048\n",
      "using loss log_loss,  penalty is None,  alpha is 0.1, the acc is 0.898\n",
      "[[272  29]\n",
      " [ 31 255]]\n",
      "0.8977853492333902\n",
      "using loss log_loss,  penalty is None,  alpha is 1, the acc is 0.876\n",
      "[[274  27]\n",
      " [ 46 240]]\n",
      "0.8756388415672913\n",
      "using loss log_loss,  penalty is None,  alpha is 5, the acc is 0.712\n",
      "[[298   3]\n",
      " [166 120]]\n",
      "0.7120954003407155\n",
      "using loss log_loss,  penalty is None,  alpha is 10, the acc is 0.521\n",
      "[[301   0]\n",
      " [281   5]]\n",
      "0.5212947189097104\n",
      "using loss log,  penalty is l1,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log,  penalty is l1,  alpha is 0.1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss log,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l2,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log,  penalty is l2,  alpha is 0.1, the acc is 0.898\n",
      "[[272  29]\n",
      " [ 31 255]]\n",
      "0.8977853492333902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss log,  penalty is l2,  alpha is 1, the acc is 0.802\n",
      "[[291  10]\n",
      " [106 180]]\n",
      "0.8023850085178875\n",
      "using loss log,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is None,  alpha is 0.001, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "0.9114139693356048\n",
      "using loss log,  penalty is None,  alpha is 0.1, the acc is 0.898\n",
      "[[272  29]\n",
      " [ 31 255]]\n",
      "0.8977853492333902\n",
      "using loss log,  penalty is None,  alpha is 1, the acc is 0.876\n",
      "[[274  27]\n",
      " [ 46 240]]\n",
      "0.8756388415672913\n",
      "using loss log,  penalty is None,  alpha is 5, the acc is 0.712\n",
      "[[298   3]\n",
      " [166 120]]\n",
      "0.7120954003407155\n",
      "using loss log,  penalty is None,  alpha is 10, the acc is 0.521\n",
      "[[301   0]\n",
      " [281   5]]\n",
      "0.5212947189097104\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.001, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "0.9114139693356048\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.1, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss modified_huber,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss modified_huber,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss modified_huber,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.1, the acc is 0.893\n",
      "[[274  27]\n",
      " [ 36 250]]\n",
      "0.8926746166950597\n",
      "using loss modified_huber,  penalty is l2,  alpha is 1, the acc is 0.894\n",
      "[[272  29]\n",
      " [ 33 253]]\n",
      "0.8943781942078365\n",
      "using loss modified_huber,  penalty is l2,  alpha is 5, the acc is 0.842\n",
      "[[279  22]\n",
      " [ 71 215]]\n",
      "0.8415672913117547\n",
      "using loss modified_huber,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.001, the acc is 0.911\n",
      "[[280  21]\n",
      " [ 31 255]]\n",
      "0.9114139693356048\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss modified_huber,  penalty is None,  alpha is 1, the acc is 0.874\n",
      "[[277  24]\n",
      " [ 50 236]]\n",
      "0.8739352640545145\n",
      "using loss modified_huber,  penalty is None,  alpha is 5, the acc is 0.860\n",
      "[[274  27]\n",
      " [ 55 231]]\n",
      "0.8603066439522998\n",
      "using loss modified_huber,  penalty is None,  alpha is 10, the acc is 0.843\n",
      "[[275  26]\n",
      " [ 66 220]]\n",
      "0.8432708688245315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss squared_hinge,  penalty is l1,  alpha is 0.001, the acc is 0.881\n",
      "[[276  25]\n",
      " [ 45 241]]\n",
      "0.8807495741056218\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.1, the acc is 0.898\n",
      "[[274  27]\n",
      " [ 33 253]]\n",
      "0.8977853492333902\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.001, the acc is 0.882\n",
      "[[278  23]\n",
      " [ 46 240]]\n",
      "0.8824531516183987\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.1, the acc is 0.893\n",
      "[[274  27]\n",
      " [ 36 250]]\n",
      "0.8926746166950597\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 1, the acc is 0.894\n",
      "[[272  29]\n",
      " [ 33 253]]\n",
      "0.8943781942078365\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 5, the acc is 0.842\n",
      "[[279  22]\n",
      " [ 71 215]]\n",
      "0.8415672913117547\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.001, the acc is 0.881\n",
      "[[276  25]\n",
      " [ 45 241]]\n",
      "0.8807495741056218\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.1, the acc is 0.896\n",
      "[[273  28]\n",
      " [ 33 253]]\n",
      "0.8960817717206133\n",
      "using loss squared_hinge,  penalty is None,  alpha is 1, the acc is 0.886\n",
      "[[276  25]\n",
      " [ 42 244]]\n",
      "0.8858603066439523\n",
      "using loss squared_hinge,  penalty is None,  alpha is 5, the acc is 0.860\n",
      "[[274  27]\n",
      " [ 55 231]]\n",
      "0.8603066439522998\n",
      "using loss squared_hinge,  penalty is None,  alpha is 10, the acc is 0.845\n",
      "[[274  27]\n",
      " [ 64 222]]\n",
      "0.8449744463373083\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.001, the acc is 0.903\n",
      "[[277  24]\n",
      " [ 33 253]]\n",
      "0.9028960817717206\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss perceptron,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss perceptron,  penalty is l1,  alpha is 5, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss perceptron,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.001, the acc is 0.853\n",
      "[[290  11]\n",
      " [ 75 211]]\n",
      "0.8534923339011925\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.1, the acc is 0.588\n",
      "[[ 63 238]\n",
      " [  4 282]]\n",
      "0.5877342419080068\n",
      "using loss perceptron,  penalty is l2,  alpha is 1, the acc is 0.588\n",
      "[[ 63 238]\n",
      " [  4 282]]\n",
      "0.5877342419080068\n",
      "using loss perceptron,  penalty is l2,  alpha is 5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss perceptron,  penalty is l2,  alpha is 10, the acc is 0.521\n",
      "[[301   0]\n",
      " [281   5]]\n",
      "0.5212947189097104\n",
      "using loss perceptron,  penalty is None,  alpha is 0.001, the acc is 0.899\n",
      "[[272  29]\n",
      " [ 30 256]]\n",
      "0.899488926746167\n",
      "using loss perceptron,  penalty is None,  alpha is 0.1, the acc is 0.899\n",
      "[[277  24]\n",
      " [ 35 251]]\n",
      "0.899488926746167\n",
      "using loss perceptron,  penalty is None,  alpha is 1, the acc is 0.860\n",
      "[[274  27]\n",
      " [ 55 231]]\n",
      "0.8603066439522998\n",
      "using loss perceptron,  penalty is None,  alpha is 5, the acc is 0.840\n",
      "[[275  26]\n",
      " [ 68 218]]\n",
      "0.8398637137989778\n",
      "using loss perceptron,  penalty is None,  alpha is 10, the acc is 0.736\n",
      "[[291  10]\n",
      " [145 141]]\n",
      "0.7359454855195912\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.001, the acc is 0.899\n",
      "[[282  19]\n",
      " [ 40 246]]\n",
      "0.899488926746167\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.1, the acc is 0.840\n",
      "[[291  10]\n",
      " [ 84 202]]\n",
      "0.8398637137989778\n",
      "using loss squared_error,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_error,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.001, the acc is 0.899\n",
      "[[282  19]\n",
      " [ 40 246]]\n",
      "0.899488926746167\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_error,  penalty is l2,  alpha is 1, the acc is 0.894\n",
      "[[269  32]\n",
      " [ 30 256]]\n",
      "0.8943781942078365\n",
      "using loss squared_error,  penalty is l2,  alpha is 5, the acc is 0.530\n",
      "[[301   0]\n",
      " [276  10]]\n",
      "0.5298126064735945\n",
      "using loss squared_error,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is None,  alpha is 0.001, the acc is 0.899\n",
      "[[282  19]\n",
      " [ 40 246]]\n",
      "0.899488926746167\n",
      "using loss squared_error,  penalty is None,  alpha is 0.1, the acc is 0.899\n",
      "[[282  19]\n",
      " [ 40 246]]\n",
      "0.899488926746167\n",
      "using loss squared_error,  penalty is None,  alpha is 1, the acc is 0.886\n",
      "[[276  25]\n",
      " [ 42 244]]\n",
      "0.8858603066439523\n",
      "using loss squared_error,  penalty is None,  alpha is 5, the acc is 0.864\n",
      "[[274  27]\n",
      " [ 53 233]]\n",
      "0.8637137989778535\n",
      "using loss squared_error,  penalty is None,  alpha is 10, the acc is 0.772\n",
      "[[291  10]\n",
      " [124 162]]\n",
      "0.7717206132879046\n",
      "using loss huber,  penalty is l1,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss huber,  penalty is l1,  alpha is 0.1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 0.001, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss huber,  penalty is l2,  alpha is 0.1, the acc is 0.712\n",
      "[[300   1]\n",
      " [168 118]]\n",
      "0.7120954003407155\n",
      "using loss huber,  penalty is l2,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is None,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss huber,  penalty is None,  alpha is 0.1, the acc is 0.876\n",
      "[[278  23]\n",
      " [ 50 236]]\n",
      "0.8756388415672913\n",
      "using loss huber,  penalty is None,  alpha is 1, the acc is 0.727\n",
      "[[297   4]\n",
      " [156 130]]\n",
      "0.727427597955707\n",
      "using loss huber,  penalty is None,  alpha is 5, the acc is 0.514\n",
      "[[301   0]\n",
      " [285   1]]\n",
      "0.514480408858603\n",
      "using loss huber,  penalty is None,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.726\n",
      "[[293   8]\n",
      " [153 133]]\n",
      "0.7257240204429302\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.811\n",
      "[[290  11]\n",
      " [100 186]]\n",
      "0.8109028960817717\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.555\n",
      "[[301   0]\n",
      " [261  25]]\n",
      "0.555366269165247\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.893\n",
      "[[283  18]\n",
      " [ 45 241]]\n",
      "0.8926746166950597\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.821\n",
      "[[287  14]\n",
      " [ 91 195]]\n",
      "0.8211243611584327\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.540\n",
      "[[301   0]\n",
      " [270  16]]\n",
      "0.5400340715502555\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.899\n",
      "[[282  19]\n",
      " [ 40 246]]\n",
      "0.899488926746167\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.893\n",
      "[[282  19]\n",
      " [ 44 242]]\n",
      "0.8926746166950597\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.899\n",
      "[[282  19]\n",
      " [ 40 246]]\n",
      "0.899488926746167\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.894\n",
      "[[272  29]\n",
      " [ 33 253]]\n",
      "0.8943781942078365\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.888\n",
      "[[278  23]\n",
      " [ 43 243]]\n",
      "0.8875638841567292\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.516\n",
      "[[301   0]\n",
      " [284   2]]\n",
      "0.5161839863713799\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.899\n",
      "[[282  19]\n",
      " [ 40 246]]\n",
      "0.899488926746167\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.896\n",
      "[[282  19]\n",
      " [ 42 244]]\n",
      "0.8960817717206133\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.889\n",
      "[[281  20]\n",
      " [ 45 241]]\n",
      "0.889267461669506\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.872\n",
      "[[274  27]\n",
      " [ 48 238]]\n",
      "0.8722316865417377\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.859\n",
      "[[274  27]\n",
      " [ 56 230]]\n",
      "0.858603066439523\n"
     ]
    }
   ],
   "source": [
    "#SGDClassification\n",
    "X=dfNew.drop(columns = ['AUTHOR','CONTENT','CLASS'])\n",
    "y=dfNew['CLASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "losses = ['hinge', 'log_loss', 'log', 'modified_huber', \n",
    "          'squared_hinge', 'perceptron', 'squared_error',\n",
    "          'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalties = ['l1','l2',None]\n",
    "alphas = [0.001, .1, 1, 5, 10]\n",
    "for _loss in losses: \n",
    "    for _penalty in penalties:\n",
    "        for _alpha in alphas:\n",
    "            model = SGDClassifier(loss=_loss, penalty=_penalty, alpha=_alpha, random_state=10)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            print(\"using loss {}\".format(_loss) + \",  penalty is {}\".format(_penalty) +\n",
    "                  \",  alpha is {}\".format(_alpha) + \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "            print(confusion_matrix(y_test, pred))\n",
    "            print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1642b6ef-1f1e-4569-9f46-3c337a55c156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[281  20]\n",
      " [ 33 253]]\n",
      "0.909710391822828\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier:\n",
    "model = DecisionTreeClassifier(random_state = 10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "42417827-2a04-4ed7-8a18-13c68bb2f3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[281  20]\n",
      " [ 31 255]]\n",
      "0.9131175468483816\n"
     ]
    }
   ],
   "source": [
    "#SVC with kernel=poly and C=5 had highest acc\n",
    "model = SVC(kernel='poly', C=5, probability = True, random_state=10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6f0abd66-3a44-4108-ade5-c264235e5db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAy7klEQVR4nO3dd5xU1fnH8c8jvSOgKCIRA9IUUBFL7EQEC2ps2FB/GmOwG409GlFjix0LEoMVosSCFWLvUaJIFUSF3aUoRZEidZ/fH2fWXZbd2dnduVO/79drXjvlzp1nL8t95pxzz3PM3REREanMZukOQEREMpsShYiIxKVEISIicSlRiIhIXEoUIiISlxKFiIjEpUQhIiJxKVGIiEhcShSSc8xsjpn9bGYrzGyhmY0ys6blttnLzN40s+VmtszMXjSz7uW2aW5md5lZQWxfs2OP28T57L3M7MM4rzeJ7euVCl5zM+tU7rnrzOyJ2sQkUltKFJKrDnf3pkBvYGfgipIXzGxPYALwAtAO6Ah8AXxgZtvHtqkPvAH0AAYAzYG9gCVA3zifewiwSRIo4xhgDdDfzLauzi9Ui5hEaqVuugMQiZK7LzSz8YSEUeJW4DF3v7vMc1eb2a7AdcCQ2K0DcIC7r4ht8z0wrIqPPAQ4M87rpwIPAgOBk4DbE/tNoBYxidSKWhSS08ysPeGkPDv2uDHhW/gzFWz+NHBQ7P5vgdfKnJAT+aytgbbA55W83gHYH3gydhuS6L5rGpNIMihRSK563syWA4WEb93Xxp5vRfi7X1DBexYAJX39rSvZJp5DCCfyyiptDgEmu/t0YDTQw8x2rsb+axKTSK0pUUiuOtLdmxG+wXelNAH8ABQDFY0PbA0sjt1fUsk28VQ1PjGE0JLA3ecD7xC6okpsAOqVe089YF0tYhKpNSUKyWnu/g4withYgLuvBD4Cjq1g8+MIg8UArwMHm1mTRD7HzOoB+wH/qeT1vYDOwBWxK7EWArsDJ5hZyVhhAbBdubd2BObWJCaRZFGikHxwF3CQmfWOPb4cONXMzjezZma2uZndAOwJ/DW2zeOEbqt/m1lXM9vMzFqb2ZVmdkgFn7EPoVvpp0piOJWQRLoTBtZ7AzsCjQljKAD/Igyqt4993m+Bw4GxNYxJJCmUKCTnufsi4DHgmtjj94GDgd8R+vznEi6h3dvdv4pts4YwePwl4QT/E/AJoQvrvxV8TKXdTmbWkNBaudfdF5a5fUs4+Zd0P10PfAi8T+giuxU4yd2n1jAmkaQwrXAnUntmNh04JjZQLZJTImtRmNkjZva9mU2t5HUzs3tiM0snm9kuUcUiEqXYRLjHlCQkV0XZ9TSKMHu0MgMJg3udgbOAByKMRSQy7r7W3W9OdxwiUYksUbj7u8DSOJscQfgW5u7+MdCyuiUNREQkeuks4bEN4QqOEkWx5zaZUGRmZxFaHTRp0mTXrl27piRAEZF0W7++9LZhA6xbF36Wfb78NmWHnrdiAVuzkM8pXuzuW9QkhnQmCqvguQpH1t19BDACoE+fPj5x4sQo4xIRSbq1a2HJko1vixfHv//DDxuf9MuqWxfatIG2baF163Br06bM/dZO6zZGp+nj2GryBFo9NXxuxXuqWjoTRRGwbZnH7YH5aYpFRCRhq1ZtfGJP5KS/fHnl+2vUaOOT/M47lzvpV3C/WTOwir5u//ADXHIJbL89nH4VHD4IGARPDa/x75vORDEOONfMxhBmqC5zd9WxEZGUcYeffqr8JF/ZSX/16sr32bx56cm8TRvo2rX0JF/ZSb9RoyT9Qs89B0OHwqJFcPXVSdpphInCzEYT6uy0MbMiQlG2egDu/iBhctIhhKqeq4DTo4pFRHLfhg3hy3RV3+zLPl66NPTrV2SzzWDzzUtP5h06wC67VHzCL3ncqhXUK1+tKxW++w7OOw+eeQZ694aXXw7BJklkicLdT6jidQfOierzRSR7lfTnV+ek/+OPlffn16u38cm9a9equ3ZatgzJIisUFobkcOONcOmlSc9WWrhIRCLjHvrzEz3pl9xfEWfFjcaNNz6x/+pXVZ/0mzatpD8/m82dCy++COeeC336QEFB+GUjoEQhIglxh2XLqj+Iu2ZN5fts0aL0ZL7lltC9e9X9+Q0bpu53zkjFxfDAA3D55eHx0UfD1ltHliRAiUIkL61fv2l/flUn/aVLwzhARTbbLPTPl5zMt9sufMmtqj+/rs5A1TNzJpx5Jrz/Phx8MDz0UEgSEdM/k0iWW7MmsT788v35lalff+OTe/fuVXfttGiRRf352WrVKth775CtR42CIUNS1p+mRCGSIdxh5crqD+KuXFn5Pps02fjE3rFj1Sf9Jk1ysD8/m82aBZ07h8GZxx8PVzVttVVKQ1CiEIlAcXH1+vNLHq9dW/k+W7YsPZlvtRX06FH1Sb9Bg5T9ypJsq1fDsGFwyy2hBXHyyTAgXp3V6ChRiFRh/frQP1/d/vzi4or3V6fOxv35228PffvGH8TdfHP15+eVDz6AM84IYxKnnw6HHprWcPSnJ3ll9erq9+cvW1b5/ho02PjEvuOOlX/LL3ncvLn68yWOYcPg2mvDDL/x46F//3RHpEQh2ck9XGtf3ZP+qlWV77Np041P7L/+ddVdO40bqz9fksQ9/DH17h1mWd94Y/ijzABKFJJ2xcXhKpzqDuKuW1f5PsuWXmjXDnbaKf5Jv1Ur9edLmixdChddBJ06wTXXwOGHh1sGUaKQpFq3LvzdV2cm7g8/xO/PL3ti79QJ9tij6v78OnVS+3uL1MjYsXDOOeE/zTXXpDuaSilRSKV+/jnxapol93/6qfL9NWy48Ym9Z8/E+vPVtSM5Z8GCUHrj2Wdh111hwgTo1SvdUVVKiSIPuIda+NU96f/8c+X7bNZs45N7586J9eeLCDB/fhiovuUWuPjijL+kLbOjk00UF4eumuoM4i5ZUnl/vtnG/fnt24cvNlX159evn9rfWyTrzZkTividd15oRRQWhv98WUCJIo3WrUu8sFrJbenS+Esjlj2x77BDxd05Ze+3bKn+fJFIbdgAw4fDlVeG66KPPTbMmMySJAFKFElTUkq5Oif9eP35jRptfGLfdtuq+/MrXRpRRNJjxoxQxO/DD8Os6oceSnn5jWRQoiinZGnE6pz0Fy+uemnEkpN5mzbQpUv8k77680VywKpVsO++ob/4scdCCY4s/SanRAF8/XVoDc6fH078lS2NaLZx6YUOHapeBD1tSyOKSHp8+WX4Nti4MTz5ZBj0a9s23VHVihIF8O678PnnIeFvu238/nyVXhCRCv38M1x3Hdx+Ozz6aDihZED5jWRQoiBcfAAwcqRm54pIDbz7bhiL+Oqr8POww9IdUVLp+zFhqdm2bZUkRKQG/vpX2G+/0Gf9+uvw8MOh+yGHKFEQWhQdOqQ7ChHJKiXXqffpE2o1TZkC/fqlN6aIKFEQWhTbbpvuKEQkKyxeDKecEsqBQ1gr4o47wtKAOSrvE4V7aFEoUYhIXO7w9NNhEfExY/Lqypa8H8z+8cew5rC6nkSkUvPnw9Ch8MILoavp9ddDVcs8kT8psRIFBeGnWhQiUqmFC+HNN+G22+Cjj/IqSYBaFL9cGqtEISIb+eYbGDcOLrwQdtklfKvMsauZEpX3LYqSRKGuJxEBQhG/O+8MC6Bfe21oTUDeJglQoqCgIFRdzfIZ9iKSDNOmwW9+E9aIOPDA8DgLi/glm7qeCsMaDCq1LZLnVq0KE+fM4KmnYPDgrC3il2x5nyg0h0Ikz02fDt26hSJ+Y8aEIn5bbJHuqDJK3nc9aQ6FSJ5atQouvRR22gmeeCI899vfKklUIK9bFBs2wLx5GsgWyTtvvw2//z3Mng1/+AMMGpTuiDJaXrcovvsuLEeqFoVIHrn2WjjggDDT+s034cEHoUWLdEeV0fI6UWgOhUgeKSni17cv/OlPMHlySBhSpUgThZkNMLOZZjbbzC6v4PUWZvaimX1hZtPM7PQo4ylPcyhE8sCiRXDiiXD99eHxoYeGxYW03nDCIksUZlYHGA4MBLoDJ5hZ93KbnQNMd/dewP7A382sflQxlafyHSI5zD1c5tqtG4wdC/VTdmrJOVG2KPoCs939G3dfC4wBjii3jQPNzMyApsBSoJIVq5OvsDB8qdh881R9ooikRFFRGKA+6STo1CmsdXzFFemOKmtFmSi2AQrLPC6KPVfWfUA3YD4wBbjA3YvL78jMzjKziWY2cdGiRUkLsKAgdDtpTo1Ijlm0KCxPescd8MEH0KNHuiPKalEmiopOv17u8cHAJKAd0Bu4z8yab/Im9xHu3sfd+2yRxGucNYdCJIfMnh1qNAHsvHP4D37RRSq7kARRJooioOxpuD2h5VDW6cCzHswGvgW6RhjTRrQEqkgOWL8+DE7vtFNYv/q778LzzTf5zik1FGWi+BTobGYdYwPUg4Fx5bYpAPoBmFlboAvwTYQx/WLNmlAUUi0KkSw2ZQrstVeYYd2/fyjipwqfSRfZzGx3X29m5wLjgTrAI+4+zczOjr3+IDAMGGVmUwhdVZe5++KoYipr3rzwU4lCJEutWhXmQWy2WajRdNxxGnCMSKQlPNz9FeCVcs89WOb+fKB/lDFURnMoRLLU1KlhcLpxY/jXv0IRvzZt0h1VTsvbmdmaQyGSZVauDOtE9OxZWsSvXz8liRTI26KAKt8hkkXeeCMU8fv2Wxg6FI4oPyVLopTXLYrWrTWLXyTjXXNNKP9dty688w4MH64rmlIsbxOF5lCIZLji2NzbvfaCP/8ZvvgC9t03vTHlqbxOFBrIFslA338fliH961/D44ED4ZZboFGj9MaVx/I2UWgJVJEM4x4Gqbt1g+eeU79wBsnLRLF8OSxbpkQhkjEKC+Gww+CUU6BLl1DE77LL0h2VxORlotAcCpEMs2RJKN53993w3nvQvfyKBJJOeXl5rOZQiGSAWbNg3Di45BLo3Tt8g2vWLN1RSQXyukWhRCGSBuvXh8Hpnj3hxhtLi/gpSWSsvEwUBQWhPEy7dumORCTPfPEF7L47XH45HHIITJ+uIn5ZIC+7ngoLYeutoV69dEcikkdWrQolN+rWDUuTHn10uiOSBOVtotBAtkiKTJ4c1opo3BieeSYU8WvVKt1RSTXkbdeTxidEIrZiBVxwQRiofvzx8NwBByhJZKG8SxTuYd11JQqRCP3nP6EVcc89cM45cNRR6Y5IaiHvEsXixbB6tbqeRCJz1VVhtbkGDcKciHvv1RVNWS7hRGFmTaIMJFU0h0IkIiVF/PbeG664AiZNCvcl61WZKMxsLzObDsyIPe5lZvdHHllENIdCJMkWLoRjjoHrrguPBw6Em26Chg3TGpYkTyItijuBg4ElAO7+BZC1tX5VvkMkSdxh1KhQbuOll7RGRA5L6PJYdy+0jRct3xBNONErKAhdp1tske5IRLLY3Llw1lkwYULoXho5MhTzk5yUSIui0Mz2AtzM6pvZJcS6obJRyYJFG+c9EamWH3+ETz+F++4Lq84pSeS0RFoUZwN3A9sARcAEYGiUQUVJcyhEamjmzFDE79JLw6S5ggJo2jTdUUkKJNKi6OLuJ7l7W3ff0t1PBrpFHVhUtASqSDWtWwd/+1tIDjffHFagAyWJPJJIorg3wecy3vr1MH++BrJFEvb556GI35VXwuGHhyJ+W26Z7qgkxSrtejKzPYG9gC3M7OIyLzUH6kQdWBTmzw+XeqtFIZKAVavgoINC9cx//xt+97t0RyRpEm+Moj7QNLZN2WmVPwHHRBlUVDSHQiQBn38e6jM1bhyqvPbqBZtvnu6oJI0qTRTu/g7wjpmNcve5KYwpMppDIRLH8uVhRvXw4fDoozBkCOy/f7qjkgyQyFVPq8zsNqAH8MtUS3c/MLKoIqLyHSKVeO01+MMfwrepCy5QN5NsJJHB7CeBL4GOwF+BOcCnEcYUmcJCaNFCE0hFNnLFFaHsRpMm8MEHcNdduqJJNpJIi6K1u//DzC4o0x31TtSBRUFzKETK2LAB6tQJ3Ut168LVV4eyBSLlJJIo1sV+LjCzQ4H5QPvoQoqO5lCIAAsWhDUievSAYcPg4IPDTaQSiXQ93WBmLYA/AZcAI4ELowwqKloCVfKaO/zzn6GI36uv6komSViVLQp3fyl2dxlwAICZ/SbKoKKwalVYtEgtCslLc+bA738Pr78O++wTivjtsEO6o5IsEW/CXR3gOEKNp9fcfaqZHQZcCTQCdk5NiMlRVBR+KlFIXlq2DD77DO6/P1zdtFneLW4ptRDvr+UfwJlAa+AeM/sncDtwq7snlCTMbICZzTSz2WZ2eSXb7G9mk8xsWpSD5JpDIXln+vRQmwlKi/j98Y9KElJt8bqe+gA93b3YzBoCi4FO7r4wkR3HWiTDgYMIVWc/NbNx7j69zDYtgfuBAe5eYGaRFZHRHArJG2vXwq23hoHqZs3g//4v1GdqkhOrGUsaxPtqsdbdiwHcfTUwK9EkEdMXmO3u37j7WmAMcES5bU4EnnX3gtjnfF+N/VdLSYuifVZeryWSoIkTYbfd4JprwqQ5FfGTJIjXouhqZpNj9w34deyxAe7uPavY9zZAYZnHRcDu5bbZAahnZm8T6knd7e6Pld+RmZ0FnAXQoYZ9RwUF0LatLhOXHLZyZbjMtWFDeOEFGDQo3RFJjoiXKGq75kRFa8h5BZ+/K9CPMED+kZl97O6zNnqT+whgBECfPn3K7yMhmkMhOeuzz0IRvyZN4LnnoGdPaNky3VFJDqm068nd58a7JbDvIqDsqbk9YbJe+W1ec/eV7r4YeBfoVd1fIhGaQyE556efYOhQ2HVXeOKJ8Ny++ypJSNJFefnDp0BnM+toZvWBwcC4ctu8AOxjZnXNrDGhayrp63G7q3yH5JhXXgkzqx96CC6+GI4+Ot0RSQ5LpIRHjbj7ejM7FxhPWOjoEXefZmZnx15/0N1nmNlrwGSgGBjp7lOTHcuPP4buWyUKyQmXXRauaurePawXsXv5oT+R5EooUZhZI6CDu8+szs7d/RXglXLPPVju8W3AbdXZb3VpDoVkPfewPGOdOtCvXxiwvvJKXZ0hKVFl15OZHQ5MAl6LPe5tZuW7kDKa5lBIVps3D448Eq69Njzu3x/++lclCUmZRMYoriPMifgRwN0nAdtFFVAU1KKQrOQODz8cupgmTIA2bdIdkeSpRLqe1rv7MrOKrnbNDgUFodx+27bpjkQkQd9+C2ecAW+9FdaLePhh6NQp3VFJnkokUUw1sxOBOmbWGTgf+DDasJKrsBC22SZ074pkhRUrYPLkcFXTmWeqPpOkVSJ/fecR1steAzxFKDd+YYQxJZ3mUEhWmDoVbrop3N9pp9AUPussJQlJu0T+Aru4+1XuvlvsdnWs9lPW0BwKyWhr14bB6V12gTvvhO9jJc8aN05vXCIxiSSKO8zsSzMbZmY9Io8oyYqLw0UjShSSkT79NMysvu46OPZYFfGTjJTICncHmNlWhEWMRphZc+Bf7n5D5NElwXffwbp16nqSDLRyJQwYAI0awbhxcPjh6Y5IpEIJdX66+0J3vwc4mzCn4i9RBpVMmkMhGWfixNDUbdIkVHmdNk1JQjJaIhPuupnZdWY2FbiPcMVT1qzqoDkUkjGWLQvLkO62W2kRv733hhYt0huXSBUSuTz2n8BooL+7l6/+mvHUopCM8OKLcPbZsHAhXHIJHHNMuiMSSVgiYxR7pCKQqBQWhotHNt883ZFI3rr0Urj99nDJ6/PPhxaFSBapNFGY2dPufpyZTWHjBYcSXeEuI5TMocjiieWSjdxhw4ZQEqB/f2jePFR9rV8/3ZGJVFu8FsUFsZ+HpSKQqGgOhaRcURH88Y9hpbkbb4SDDgo3kSwVb4W7BbG7QytY3W5oasKrPS2BKilTXBxKbnTvDm++CVttle6IRJIikctjK/oqNDDZgURhzZowdqgrniRy33wDBx4YBqz79oUpU+C889IdlUhSxBuj+COh5bC9mU0u81Iz4IOoA0uGefPCT7UoJHIrV4ZZ1SNHwv/9nwbFJKfEG6N4CngV+BtweZnnl7v70kijShLNoZBITZkSJsxdfXW4omnu3DDLWiTHxOt6cnefA5wDLC9zw8xaRR9a7WkOhURizRr4y19CEb977ikt4qckITmqqhbFYcD/CJfHlm1LO7B9hHElRUmLQolCkubjj8OCQtOnwymnhGqvrVunOyqRSFWaKNz9sNjPjqkLJ7kKC8P/YVVrlqRYuRIOPTTUaHrlFRiYFdd0iNRaIrWefmNmTWL3TzazO8wsK3r9NYdCkuK//y0t4vfii6GIn5KE5JFELo99AFhlZr2APwNzgccjjSpJNIdCauXHH8MypHvsUVrEb6+9oFmztIYlkmqJJIr17u7AEcDd7n434RLZjKclUKXGnn8+TJwbNSqU3jj22HRHJJI2iVSPXW5mVwCnAPuYWR2gXrRh1d7y5eELoVoUUm0XXxwGqXv1Cl1Nu+6a7ohE0iqRRHE8cCLwf+6+MDY+cVu0YdWe5lBItZQt4nfIIeEqiD//Gepl/HcikchV2fXk7guBJ4EWZnYYsNrdH4s8slrSpbGSsIKCcDXTtdeGx7/9LVx1lZKESEwiVz0dB3wCHEtYN/u/Zpbxq65osp1UqbgY7r8fevSAd96Bdu3SHZFIRkqk6+kqYDd3/x7AzLYAXgfGRhlYbRUWwmab6f++VGL27FCT6b33QgnwESNgu+3SHZVIRkokUWxWkiRilpDY1VJpVVAAW2+t3gOpxOrVMGsW/POfcOqpKuInEkciieI1MxtPWDcbwuD2K9GFlByaQyGbmDQpFPG79lrYcUeYMwcaNkx3VCIZL5HB7EuBh4CeQC9ghLtfFnVgtaU5FPKL1avD4HSfPvDAA6VF/JQkRBISbz2KzsDtwK+BKcAl7j4vVYHVhntIFIMGpTsSSbsPPwxF/L78MnQx3XEHtMqK4sciGSNei+IR4CXgaEIF2XtTElESLF4cvkSqRZHnVq6Eww+HVavgtdfCLGslCZFqizdG0czdH47dn2lmn6UioGTQHIo899FHsPvuoYjfSy+F8QjVZxKpsXgtioZmtrOZ7WJmuwCNyj2ukpkNMLOZZjbbzC6Ps91uZrYhWfMzNIciT/3wQ7jkda+94PFY3co991SSEKmleC2KBcAdZR4vLPPYgQPj7ThWE2o4cBBQBHxqZuPcfXoF290CjK9e6JVT+Y489OyzcM45sGgRXHEFHH98uiMSyRnxFi46oJb77gvMdvdvAMxsDKEC7fRy250H/BvYrZaf94uCAmjQALbYIll7lIx20UVw113Qu3dYUGjnndMdkUhOSWQeRU1tAxSWeVwE7F52AzPbBjiK0DqpNFGY2VnAWQAdEmgmFBZC+/aaQ5XTyhbxO+ww2HJLuOQSzbAUiUCUM6wrOk17ucd3AZe5+4Z4O3L3Ee7ex937bJFAM0FzKHLcnDkwYABcc0143K9f6G5SkhCJRJSJoggoO5zcHphfbps+wBgzmwMcA9xvZkfW9oO1BGqOKi6Ge+8NVzF9+CH86lfpjkgkL1TZ9WRmBpwEbO/u18fWo9jK3T+p4q2fAp3NrCMwDxhMWNfiF+7escznjAJecvfnq/UblLN+Pcyfr0SRc776Ck4/HT74ILQmHnxQiUIkRRJpUdwP7AmcEHu8nHA1U1zuvh44l3A10wzgaXefZmZnm9nZNYy3SgsWhC+e6nrKMWvXwtdfw2OPhQFrJQmRlElkMHt3d9/FzD4HcPcfzKx+Ijt391coV0DQ3R+sZNvTEtlnVTSHIod8/nko4nfddWHNiDlzwuVsIpJSibQo1sXmOjj8sh5FcaRR1YLmUOSA1avD4PRuu8FDD4W5EaAkIZImiSSKe4DngC3N7EbgfeCmSKOqBbUostz770OvXnDzzTBkCEyfrgkxImlWZdeTuz9pZv8D+hEueT3S3WdEHlkNFRZC8+bhJllmxQo44ojwjzdhQlh5TkTSLpGrnjoAq4AXyz7n7gVRBlZTmkORhd5/P9RnatoUXn45XP7atGm6oxKRmES6nl4mlBt/GXgD+AZ4NcqgakNzKLLIkiWhe2mffUqL+O2xh5KESIZJpOtpp7KPY5Vj/xBZRLVUWBjGQCWDucPYsXDuubB0aZhhPXhwuqMSkUpUu9aTu39mZhl5Kv7557BokbqeMtxFF8Hdd8Ouu4axiF690h2RiMSRyBjFxWUebgbsAiyKLKJa0IJFGcw9TJuvVy+sUduuHVx8cSjqJyIZLZEximZlbg0IYxVHRBlUTWkORYb69lvo37+0iN+BB8Kf/6wkIZIl4v5PjU20a+rul6YonlrRHIoMs2ED3HcfXHkl1KkDxx6b7ohEpAYqTRRmVtfd1ye67GkmKGlRtG+f3jgEmDULTjstrF89cGCYYa0MLpKV4rUoPiGMR0wys3HAM8DKkhfd/dmIY6u2wkJo21aVHjLC+vUwdy488QSceKJWkRLJYol0ErcClhBWoXPC7GwHMi5RaA5Fmk2cGIr4DRsG3bvDN98oa4vkgHiD2VvGrniaCkyJ/ZwW+zk1BbFVW2GhEkVa/PxzGJzefXd45BEV8RPJMfESRR2gaezWrMz9kltGcVf5jrR45x3o2RNuuw3OOAOmTVMRP5EcE6/raYG7X5+ySGrpxx9DTTm1KFJoxQr43e+gZUt4441w2auI5Jx4iSKrRh81hyKF3nsPfvObUJPp1VfDokJNmqQ7KhGJSLyup34piyIJNIciBRYvhpNPhn33LS3i17evkoRIjqu0ReHuS1MZSG2pfEeE3OHpp+G88+CHH+Daa1XETySP5EwNhcLCUBFiq63SHUkOuuACuPfeUJb3jTdgp52qfo+I5IycSRQFBbDNNqFShCSBO6xbB/Xrw1FHwa9+BRdeqAMskocSKQqYFTSHIom+/hr69YOrrw6PDzgA/vQnJQmRPJVTiUJXPNXShg1wxx2ha+l//4MuXdIdkYhkgJzoeiouhqIitShq5csv4dRT4ZNP4PDD4YEHQl+eiOS9nEgU330XutPVoqiF4mKYPx9Gj4bjj1cRPxH5RU4kCs2hqKFPPglF/G68MRTx+/rrMHgtIlJGToxRaA5FNa1aBZdcAnvuCY8+WlrET0lCRCqQU4lCXU8JeOutMFj997/D73+vIn4iUqWc6Xpq3Bg23zzdkWS4FSvCcqQtW4aEsf/+6Y5IRLJAzrQott1W46+VevvtMFhdUsRv8mQlCRFJWM4kCnU7VWDRIjjhhDBh7oknwnO77RaaXyIiCcqJRKElUMtxh6eegm7d4Nlnw9KkKuInIjWU9WMUa9bAwoVqUWzkvPNg+HDYYw/4xz/Cpa8iIjWU9Yli3rzwM+9bFMXFsH59uMT1mGOgU6eQMFSfSURqKdKuJzMbYGYzzWy2mV1ewesnmdnk2O1DM+tV3c/QHArgq6/CMqRXXRUe77+/Kr2KSNJElijMrA4wHBgIdAdOMLPyfSDfAvu5e09gGDCiup+T13Mo1q+H22+Hnj1h0qQwJiEikmRRdj31BWa7+zcAZjYGOAKYXrKBu39YZvuPgfbV/ZC8Ld8xYwYMGQITJ8IRR8D990O7dumOSkRyUJRdT9sAhWUeF8Weq8wZwKsVvWBmZ5nZRDObuKik3ERMYSG0apWnV3x+9x3861/w3HNKEiISmSgTRUXT37zCDc0OICSKyyp63d1HuHsfd++zRblyE3k1h+Ljj+GKK8L9bt1CEb/jjtNMQxGJVJSJoggo2yHUHphffiMz6wmMBI5w9yXV/ZC8mEOxciVcdBHstRc8+WRpEb969dIbl4jkhSgTxadAZzPraGb1gcHAuLIbmFkH4FngFHefVZMPyfkWxeuvw447wl13wdChKuInIikX2WC2u683s3OB8UAd4BF3n2ZmZ8defxD4C9AauN9C98l6d++T6GcsXw4//pjDLYoVK8KM6lat4N13YZ990h2RiOShSCfcufsrwCvlnnuwzP0zgTNruv+cnUPx5puw336hiN/48WFmdaNG6Y5KRPJUVtd6yrk5FN99Fwan+/UrLeK3665KEiKSVlmdKHJmDoU7PP54aDmULE164onpjkpEBMjyWk+FheHK0KyfQnDOOfDAA2Fp0n/8QzOsRSSjZH2iaNcuS68SLS6GdeugQQM4/viQHIYOVX0mEck4Wd/1lJXdTjNnhsHqkiJ+++2nSq8ikrGyOlFk3RyKdevg5puhVy+YOhV22indEYmIVClrE4V76VrZWWHaNNh991CC49BDQ1G/U09Nd1QiIlXK2jGKxYth9eosShR16sDSpTB2LBx9dLqjERFJWNa2KLJiDsWHH8JlsTqHXbvC7NlKEiKSdbI2UWT0HIoVK+D882HvvUMZ8MWLw/N1s7YBJyJ5LGsTRcaW75gwIRTxu+8+OPfcMGjdpk26oxIRqbGs/YpbWBimIGRUIdUVK+Ckk6B1a3jvPfjNb9IdkYhIrWVti6KgANq3h80y4Tf4z39gw4ZQxG/ChLB+tZKEiOSITDjN1khGzKFYsCAMTvfvHxYUAth5Z2jYML1xiYgkUVYnirSNT7jDqFGhiN/LL4dJdCriJyI5KivHKNavh3nz0pgo/vhHeOihcFXTyJHQpUuaAhHJbOvWraOoqIjVq1enO5S80bBhQ9q3b0+9JBbBy8pEsWBBqKmX0q6nskX8TjwRevaEs8/OkEESkcxUVFREs2bN2G677YitYikRcneWLFlCUVERHTt2TNp+s/Isl/I5FDNmhGVIr7wyPN5331DpVUlCJK7Vq1fTunVrJYkUMTNat26d9BZcVp7pUjaHYt06uOkm6N0bvvwyDFSLSLUoSaRWFMc7K7ueUlK+Y9o0OPnkcKnrscfCvfdC27YRfqCISGbKyhZFQQE0bx5ukalbF5Ytg2efhaefVpIQyWLPPfccZsaXX375y3Nvv/02hx122EbbnXbaaYwdOxYIA/GXX345nTt3Zscdd6Rv3768+uqrtY7lb3/7G506daJLly6MHz++wm2++OIL9txzT3baaScOP/xwfvrpJwCWLFnCAQccQNOmTTn33HNrHUuisjJRRDaH4r334JJLwv0uXWDWLDjqqAg+SERSafTo0ey9996MGTMm4fdcc801LFiwgKlTpzJ16lRefPFFli9fXqs4pk+fzpgxY5g2bRqvvfYaQ4cOZcOGDZtsd+aZZ3LzzTczZcoUjjrqKG677TYgXNE0bNgwbr/99lrFUV1Z2/WU1PGJ5cvh8svh/vuhY8dwv00bFfETSaILLww9ucnUuzfcdVf8bVasWMEHH3zAW2+9xaBBg7juuuuq3O+qVat4+OGH+fbbb2nQoAEAbdu25bjjjqtVvC+88AKDBw+mQYMGdOzYkU6dOvHJJ5+w5557brTdzJkz2XfffQE46KCDOPjggxk2bBhNmjRh7733Zvbs2bWKo7qyskWR1CVQX30VevSABx4If8lTpqiIn0gOef755xkwYAA77LADrVq14rPPPqvyPbNnz6ZDhw40T6B/+6KLLqJ3796b3G6++eZNtp03bx7bljl5tW/fnnnz5m2y3Y477si4ceMAeOaZZygsGZhNk6z7ylxcHKp2J6XraflyGDIEttwyrB2xxx5J2KmIVKSqb/5RGT16NBdeeCEAgwcPZvTo0eyyyy6VXh1U3auG7rzzzoS3dfeEPu+RRx7h/PPP5/rrr2fQoEHUr1+/WjElW9YlirVrw88atyjcYfx4OOggaNYMXn89LCoUa16KSO5YsmQJb775JlOnTsXM2LBhA2bGrbfeSuvWrfnhhx822n7p0qW0adOGTp06UVBQwPLly2nWrFncz7jooot46623Nnl+8ODBXH755Rs91759+41aB0VFRbRr126T93bt2pUJEyYAMGvWLF5++eWEf+coZF3X07p14WeNEsWCBfC738HAgaVF/Hr1UpIQyVFjx45lyJAhzJ07lzlz5lBYWEjHjh15//336dy5M/Pnz2fGjBkAzJ07ly+++ILevXvTuHFjzjjjDM4//3zWxr6dLliwgCeeeGKTz7jzzjuZNGnSJrfySQJg0KBBjBkzhjVr1vDtt9/y1Vdf0bdv3022+/777wEoLi7mhhtu4Oyzz07mYam2rEsUJS2KanU9ucMjj0C3bvDaa3DrrSriJ5IHRo8ezVHlrlw8+uijeeqpp2jQoAFPPPEEp59+Or179+aYY45h5MiRtGjRAoAbbriBLbbYgu7du7Pjjjty5JFHskUtF8Dp0aMHxx13HN27d2fAgAEMHz6cOnXqAOFKp4kTJ/4S9w477EDXrl1p164dp59++i/72G677bj44osZNWoU7du3Z/r06bWKKRFWUZ9ZJttmmz4+f/5EVq+uRkPgD3+AESNC6Y2RI6Fz50hjFJFgxowZdOvWLd1h5J2KjruZ/c/d+9Rkf1k5RtG2bQJJYsOG0E/VsGGYYb3zznDWWarPJCJSTVl31ly7NoHxiWnTwgpzJUX89tlHlV5FRGoo686ccRPF2rUwbFhoPcyeDbvtltLYRGRT2da9ne2iON5Z2fVU4UD2lClw0knh5+DBcM89UMuBJxGpnYYNG7JkyRKVGk+RkvUoGiZ5OeasSxTFxZW0KOrXh1Wr4IUXYNCglMclIptq3749RUVFLFq0KN2h5I2SFe6SKesSBZRJFO+8A+PGwd//Hor4zZwJsUvNRCT96tWrl9SV1iQ9Ih2jMLMBZjbTzGab2SazTyy4J/b6ZDPbJZH9dmz9U1i3ev/94fnnQ00PUJIQEYlAZInCzOoAw4GBQHfgBDPrXm6zgUDn2O0s4IGq9tucZex8So8wL+Lii1XET0QkYlF2PfUFZrv7NwBmNgY4Aig7jfAI4DEPw/Qfm1lLM9va3RdUttOOzKFOqy7w3FjYffcIwxcREYg2UWwDlK2NWwSUP7NXtM02wEaJwszOIrQ4ANZsNm3aVFV6BaANsDjdQWQIHYtSOhaldCxKdanpG6NMFBVdC1f+At9EtsHdRwAjAMxsYk2noecaHYtSOhaldCxK6ViUMrOJNX1vlIPZRUDZC1nbA/NrsI2IiKRRlIniU6CzmXU0s/rAYGBcuW3GAUNiVz/tASyLNz4hIiKpF1nXk7uvN7NzgfFAHeARd59mZmfHXn8QeAU4BJgNrAJOr2x/ZYyIKORspGNRSseilI5FKR2LUjU+FllXZlxERFIr64oCiohIailRiIhIXBmbKKIq/5GNEjgWJ8WOwWQz+9DMeqUjzlSo6liU2W43M9tgZsekMr5USuRYmNn+ZjbJzKaZ2TupjjFVEvg/0sLMXjSzL2LHIpHx0KxjZo+Y2fdmNrWS12t23nT3jLsRBr+/BrYH6gNfAN3LbXMI8CphLsYewH/THXcaj8VewOax+wPz+ViU2e5NwsUSx6Q77jT+XbQkVELoEHu8ZbrjTuOxuBK4JXZ/C2ApUD/dsUdwLPYFdgGmVvJ6jc6bmdqi+KX8h7uvBUrKf5T1S/kPd/8YaGlmW6c60BSo8li4+4fu/kPs4ceE+Si5KJG/C4DzgH8D36cyuBRL5FicCDzr7gUA7p6rxyORY+FAMwuLYjQlJIr1qQ0zeu7+LuF3q0yNzpuZmigqK+1R3W1yQXV/zzMI3xhyUZXHwsy2AY4CHkxhXOmQyN/FDsDmZva2mf3PzIakLLrUSuRY3Ad0I0zonQJc4O7FqQkvo9TovJmp61EkrfxHDkj49zSzAwiJYu9II0qfRI7FXcBl7r4hx1dUS+RY1AV2BfoBjYCPzOxjd58VdXAplsixOBiYBBwI/Br4j5m95+4/RRxbpqnReTNTE4XKf5RK6Pc0s57ASGCguy9JUWyplsix6AOMiSWJNsAhZrbe3Z9PSYSpk+j/kcXuvhJYaWbvAr2AXEsUiRyL04GbPXTUzzazb4GuwCepCTFj1Oi8maldTyr/UarKY2FmHYBngVNy8NtiWVUeC3fv6O7buft2wFhgaA4mCUjs/8gLwD5mVtfMGhOqN89IcZypkMixKCC0rDCztoRKqt+kNMrMUKPzZka2KDy68h9ZJ8Fj8RegNXB/7Jv0es/BipkJHou8kMixcPcZZvYaMBkoBka6e4WXTWazBP8uhgGjzGwKofvlMnfPufLjZjYa2B9oY2ZFwLVAPajdeVMlPEREJK5M7XoSEZEMoUQhIiJxKVGIiEhcShQiIhKXEoWIiMSlRCEZKVb5dVKZ23Zxtl2RhM8bZWbfxj7rMzPbswb7GGlm3WP3ryz32oe1jTG2n5LjMjVWDbVlFdv3NrNDkvHZkr90eaxkJDNb4e5Nk71tnH2MAl5y97Fm1h+43d171mJ/tY6pqv2a2aPALHe/Mc72pwF93P3cZMci+UMtCskKZtbUzN6IfdufYmabVI01s63N7N0y37j3iT3f38w+ir33GTOr6gT+LtAp9t6LY/uaamYXxp5rYmYvx9Y2mGpmx8eef9vM+pjZzUCjWBxPxl5bEfv5r7Lf8GMtmaPNrI6Z3WZmn1pYJ+APCRyWj4gVdDOzvhbWIvk89rNLbJby9cDxsViOj8X+SOxzPq/oOIpsIt3103XTraIbsIFQxG0S8ByhikDz2GttCDNLS1rEK2I//wRcFbtfB2gW2/ZdoEns+cuAv1TweaOIrV0BHAv8l1BQbwrQhFCaehqwM3A08HCZ97aI/Xyb8O39l5jKbFMS41HAo7H79QmVPBsBZwFXx55vAEwEOlYQ54oyv98zwIDY4+ZA3dj93wL/jt0/DbivzPtvAk6O3W9JqPvUJN3/3rpl9i0jS3iIAD+7e++SB2ZWD7jJzPYllKPYBmgLLCzznk+BR2LbPu/uk8xsP6A78EGsvEl9wjfxitxmZlcDiwhVePsBz3koqoeZPQvsA7wG3G5mtxC6q96rxu/1KnCPmTUABgDvuvvPse6unla6Il8LoDPwbbn3NzKzScB2wP+A/5TZ/lEz60yoBlqvks/vDwwys0tijxsCHcjNGlCSJEoUki1OIqxMtqu7rzOzOYST3C/c/d1YIjkUeNzMbgN+AP7j7ick8BmXuvvYkgdm9tuKNnL3WWa2K6Fmzt/MbIK7X5/IL+Huq83sbULZ6+OB0SUfB5zn7uOr2MXP7t7bzFoALwHnAPcQahm95e5HxQb+367k/QYc7e4zE4lXBDRGIdmjBfB9LEkcAPyq/AZm9qvYNg8D/yAsCfkx8BszKxlzaGxmOyT4me8CR8be04TQbfSembUDVrn7E8Dtsc8pb12sZVORMYRibPsQCtkR+/nHkveY2Q6xz6yQuy8Dzgcuib2nBTAv9vJpZTZdTuiCKzEeOM9izSsz27myzxApoUQh2eJJoI+ZTSS0Lr6sYJv9gUlm9jlhHOFud19EOHGONrPJhMTRNZEPdPfPCGMXnxDGLEa6++fATsAnsS6gq4AbKnj7CGByyWB2ORMIaxu/7mHpTghriUwHPjOzqcBDVNHij8XyBaGs9q2E1s0HhPGLEm8B3UsGswktj3qx2KbGHovEpctjRUQkLrUoREQkLiUKERGJS4lCRETiUqIQEZG4lChERCQuJQoREYlLiUJEROL6f8Ot0EojHZLdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code taken from https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "#Generating ROC and AUC for Linear & C=1\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC / AUC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "af71b074-645f-40e6-a1f7-6d04f8b73bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) check                                     0.333252\n",
      " 2) com                                       0.224855\n",
      " 3) subscribe                                 0.189099\n",
      " 4) please                                    0.080526\n",
      " 5) channel                                   0.067594\n",
      " 6) youtube                                   0.047107\n",
      " 7) views                                     0.027215\n",
      " 8) song                                      0.019330\n",
      " 9) love                                      0.011023\n"
     ]
    }
   ],
   "source": [
    "#Testing importance of each attribute\n",
    "feat_labels = X.columns[:]\n",
    "\n",
    "forest = RandomForestClassifier(random_state = 10)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    if f < 15:\n",
    "        print(\"%2d) %-*s %f\" % (f + 1, 41, feat_labels[indices[f]], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "56a4c389-db77-497d-90fe-b6027f5ea411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing variables for training\n",
    "X=dfNew.drop(columns = ['AUTHOR','CONTENT','CLASS','love','song','views'])\n",
    "y=dfNew['CLASS']\n",
    "\n",
    "#Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8dc960e1-35f5-4f15-b8c8-df35224420dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using kernel linear, C value is 0.01, the acc is 0.843\n",
      "[[276  25]\n",
      " [ 67 219]]\n",
      "using kernel linear, C value is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 0.5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 10, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 0.01, the acc is 0.896\n",
      "[[269  32]\n",
      " [ 29 257]]\n",
      "using kernel rbf, C value is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 0.5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 10, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 0.01, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "using kernel poly, C value is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 0.5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel poly, C value is 10, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n"
     ]
    }
   ],
   "source": [
    "#SVC: Testing several kernal types and C values\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.01, 0.1, 0.5, 1, 5, 10]\n",
    "for kernel in kernels: \n",
    "    for c_val in C_values: \n",
    "        model = SVC(kernel=kernel, C=c_val, probability = True, random_state=10)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        print(\"using kernel {}\".format(kernel) + \", C value is {}\".format(c_val) +\n",
    "              \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "        print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e7211987-55f6-4cf1-baa6-d96772acf526",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss hinge,  penalty is l1,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss hinge,  penalty is l1,  alpha is 0.1, the acc is 0.726\n",
      "[[293   8]\n",
      " [153 133]]\n",
      "0.7257240204429302\n",
      "using loss hinge,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is l2,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss hinge,  penalty is l2,  alpha is 0.1, the acc is 0.813\n",
      "[[289  12]\n",
      " [ 98 188]]\n",
      "0.8126064735945485\n",
      "using loss hinge,  penalty is l2,  alpha is 1, the acc is 0.530\n",
      "[[301   0]\n",
      " [276  10]]\n",
      "0.5298126064735945\n",
      "using loss hinge,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss hinge,  penalty is None,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss hinge,  penalty is None,  alpha is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss hinge,  penalty is None,  alpha is 1, the acc is 0.843\n",
      "[[276  25]\n",
      " [ 67 219]]\n",
      "0.8432708688245315\n",
      "using loss hinge,  penalty is None,  alpha is 5, the acc is 0.532\n",
      "[[301   0]\n",
      " [275  11]]\n",
      "0.5315161839863713\n",
      "using loss hinge,  penalty is None,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss log_loss,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss log_loss,  penalty is l2,  alpha is 1, the acc is 0.809\n",
      "[[289  12]\n",
      " [100 186]]\n",
      "0.8091993185689949\n",
      "using loss log_loss,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is None,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log_loss,  penalty is None,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss log_loss,  penalty is None,  alpha is 1, the acc is 0.886\n",
      "[[270  31]\n",
      " [ 36 250]]\n",
      "0.8858603066439523\n",
      "using loss log_loss,  penalty is None,  alpha is 5, the acc is 0.710\n",
      "[[300   1]\n",
      " [169 117]]\n",
      "0.7103918228279387\n",
      "using loss log_loss,  penalty is None,  alpha is 10, the acc is 0.520\n",
      "[[301   0]\n",
      " [282   4]]\n",
      "0.5195911413969335\n",
      "using loss log,  penalty is l1,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log,  penalty is l1,  alpha is 0.1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss log,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l2,  alpha is 0.001, the acc is 0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log,  penalty is l2,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss log,  penalty is l2,  alpha is 1, the acc is 0.809\n",
      "[[289  12]\n",
      " [100 186]]\n",
      "0.8091993185689949\n",
      "using loss log,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is None,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss log,  penalty is None,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss log,  penalty is None,  alpha is 1, the acc is 0.886\n",
      "[[270  31]\n",
      " [ 36 250]]\n",
      "0.8858603066439523\n",
      "using loss log,  penalty is None,  alpha is 5, the acc is 0.710\n",
      "[[300   1]\n",
      " [169 117]]\n",
      "0.7103918228279387\n",
      "using loss log,  penalty is None,  alpha is 10, the acc is 0.520\n",
      "[[301   0]\n",
      " [282   4]]\n",
      "0.5195911413969335\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.1, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss modified_huber,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss modified_huber,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss modified_huber,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss modified_huber,  penalty is l2,  alpha is 1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss modified_huber,  penalty is l2,  alpha is 5, the acc is 0.813\n",
      "[[289  12]\n",
      " [ 98 188]]\n",
      "0.8126064735945485\n",
      "using loss modified_huber,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss modified_huber,  penalty is None,  alpha is 1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss modified_huber,  penalty is None,  alpha is 5, the acc is 0.886\n",
      "[[270  31]\n",
      " [ 36 250]]\n",
      "0.8858603066439523\n",
      "using loss modified_huber,  penalty is None,  alpha is 10, the acc is 0.840\n",
      "[[276  25]\n",
      " [ 69 217]]\n",
      "0.8398637137989778\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.001, the acc is 0.767\n",
      "[[288  13]\n",
      " [124 162]]\n",
      "0.7666098807495741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss squared_hinge,  penalty is l1,  alpha is 0.1, the acc is 0.898\n",
      "[[275  26]\n",
      " [ 34 252]]\n",
      "0.8977853492333902\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.001, the acc is 0.767\n",
      "[[288  13]\n",
      " [124 162]]\n",
      "0.7666098807495741\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 5, the acc is 0.843\n",
      "[[276  25]\n",
      " [ 67 219]]\n",
      "0.8432708688245315\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.001, the acc is 0.767\n",
      "[[288  13]\n",
      " [124 162]]\n",
      "0.7666098807495741\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss squared_hinge,  penalty is None,  alpha is 1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_hinge,  penalty is None,  alpha is 5, the acc is 0.886\n",
      "[[270  31]\n",
      " [ 36 250]]\n",
      "0.8858603066439523\n",
      "using loss squared_hinge,  penalty is None,  alpha is 10, the acc is 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[289  12]\n",
      " [100 186]]\n",
      "0.8091993185689949\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss perceptron,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss perceptron,  penalty is l1,  alpha is 5, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss perceptron,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.001, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.1, the acc is 0.492\n",
      "[[  5 296]\n",
      " [  2 284]]\n",
      "0.49233390119250425\n",
      "using loss perceptron,  penalty is l2,  alpha is 1, the acc is 0.492\n",
      "[[  5 296]\n",
      " [  2 284]]\n",
      "0.49233390119250425\n",
      "using loss perceptron,  penalty is l2,  alpha is 5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n",
      "using loss perceptron,  penalty is l2,  alpha is 10, the acc is 0.520\n",
      "[[301   0]\n",
      " [282   4]]\n",
      "0.5195911413969335\n",
      "using loss perceptron,  penalty is None,  alpha is 0.001, the acc is 0.896\n",
      "[[269  32]\n",
      " [ 29 257]]\n",
      "0.8960817717206133\n",
      "using loss perceptron,  penalty is None,  alpha is 0.1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss perceptron,  penalty is None,  alpha is 1, the acc is 0.896\n",
      "[[269  32]\n",
      " [ 29 257]]\n",
      "0.8960817717206133\n",
      "using loss perceptron,  penalty is None,  alpha is 5, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss perceptron,  penalty is None,  alpha is 10, the acc is 0.780\n",
      "[[289  12]\n",
      " [117 169]]\n",
      "0.7802385008517888\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.001, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.1, the acc is 0.789\n",
      "[[276  25]\n",
      " [ 99 187]]\n",
      "0.7887563884156729\n",
      "using loss squared_error,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_error,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.001, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_error,  penalty is l2,  alpha is 1, the acc is 0.896\n",
      "[[269  32]\n",
      " [ 29 257]]\n",
      "0.8960817717206133\n",
      "using loss squared_error,  penalty is l2,  alpha is 5, the acc is 0.523\n",
      "[[301   0]\n",
      " [280   6]]\n",
      "0.5229982964224872\n",
      "using loss squared_error,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is None,  alpha is 0.001, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss squared_error,  penalty is None,  alpha is 0.1, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss squared_error,  penalty is None,  alpha is 1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_error,  penalty is None,  alpha is 5, the acc is 0.843\n",
      "[[276  25]\n",
      " [ 67 219]]\n",
      "0.8432708688245315\n",
      "using loss squared_error,  penalty is None,  alpha is 10, the acc is 0.743\n",
      "[[298   3]\n",
      " [148 138]]\n",
      "0.7427597955706985\n",
      "using loss huber,  penalty is l1,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss huber,  penalty is l1,  alpha is 0.1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 0.001, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss huber,  penalty is l2,  alpha is 0.1, the acc is 0.717\n",
      "[[300   1]\n",
      " [165 121]]\n",
      "0.717206132879046\n",
      "using loss huber,  penalty is l2,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is None,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss huber,  penalty is None,  alpha is 0.1, the acc is 0.809\n",
      "[[289  12]\n",
      " [100 186]]\n",
      "0.8091993185689949\n",
      "using loss huber,  penalty is None,  alpha is 1, the acc is 0.751\n",
      "[[296   5]\n",
      " [141 145]]\n",
      "0.7512776831345827\n",
      "using loss huber,  penalty is None,  alpha is 5, the acc is 0.520\n",
      "[[301   0]\n",
      " [282   4]]\n",
      "0.5195911413969335\n",
      "using loss huber,  penalty is None,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.726\n",
      "[[293   8]\n",
      " [153 133]]\n",
      "0.7257240204429302\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.809\n",
      "[[289  12]\n",
      " [100 186]]\n",
      "0.8091993185689949\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.530\n",
      "[[301   0]\n",
      " [276  10]]\n",
      "0.5298126064735945\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.809\n",
      "[[289  12]\n",
      " [100 186]]\n",
      "0.8091993185689949\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.543\n",
      "[[301   0]\n",
      " [268  18]]\n",
      "0.5434412265758092\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.886\n",
      "[[275  26]\n",
      " [ 41 245]]\n",
      "0.8858603066439523\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.843\n",
      "[[276  25]\n",
      " [ 67 219]]\n",
      "0.8432708688245315\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.888\n",
      "[[275  26]\n",
      " [ 40 246]]\n",
      "0.8875638841567292\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.886\n",
      "[[270  31]\n",
      " [ 36 250]]\n",
      "0.8858603066439523\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.809\n",
      "[[289  12]\n",
      " [100 186]]\n",
      "0.8091993185689949\n"
     ]
    }
   ],
   "source": [
    "#SGDClassification\n",
    "X=dfNew.drop(columns = ['AUTHOR','CONTENT','CLASS','love','song','views'])\n",
    "y=dfNew['CLASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "losses = ['hinge', 'log_loss', 'log', 'modified_huber', \n",
    "          'squared_hinge', 'perceptron', 'squared_error',\n",
    "          'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalties = ['l1','l2',None]\n",
    "alphas = [0.001, .1, 1, 5, 10]\n",
    "for _loss in losses: \n",
    "    for _penalty in penalties:\n",
    "        for _alpha in alphas:\n",
    "            model = SGDClassifier(loss=_loss, penalty=_penalty, alpha=_alpha, random_state=10)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            print(\"using loss {}\".format(_loss) + \",  penalty is {}\".format(_penalty) +\n",
    "                  \",  alpha is {}\".format(_alpha) + \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "            print(confusion_matrix(y_test, pred))\n",
    "            print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "07ecfc2f-e2ee-4bb7-87c5-95dbb4c36971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier:\n",
    "model = DecisionTreeClassifier(random_state = 10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7ef1c468-1deb-4011-9ff2-9b44d00daf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code taken from https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "#Generating ROC and AUC for Linear & C=1\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC / AUC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "46961920-f45a-4f40-a342-c0b78e78466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nothing was better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d11f9236-7f72-43a0-9099-25ae19b751f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>C_LEN</th>\n",
       "      <th>A_LEN</th>\n",
       "      <th>C_SPEC</th>\n",
       "      <th>A_SPEC</th>\n",
       "      <th>check</th>\n",
       "      <th>com</th>\n",
       "      <th>please</th>\n",
       "      <th>youtube</th>\n",
       "      <th>subscribe</th>\n",
       "      <th>channel</th>\n",
       "      <th>song</th>\n",
       "      <th>love</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adam riyati</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUTHOR                                            CONTENT  \\\n",
       "0                Julius NM  Huh, anyway check out this you[tube] channel: ...   \n",
       "1              adam riyati  Hey guys check out my new channel and our firs...   \n",
       "2         Evgeny Murashkin             just for test I have to say murdev.com   \n",
       "3          ElNino Melendez   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4                   GsMega            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "...                    ...                                                ...   \n",
       "1951          Katie Mettam  I love this song because we sing it at Camp al...   \n",
       "1952  Sabina Pearson-Smith  I love this song for two reasons: 1.it is abou...   \n",
       "1953         jeffrey jules                                                wow   \n",
       "1954        Aishlin Maciel                            Shakira u are so wiredo   \n",
       "1955           Latin Bosch                         Shakira is the best dancer   \n",
       "\n",
       "      CLASS  C_LEN  A_LEN  C_SPEC  A_SPEC  check  com  please  youtube  \\\n",
       "0         1     56      9       3       0      1    0       0        0   \n",
       "1         1    166     11       7       0      1    1       1        0   \n",
       "2         1     38     16       0       0      0    1       0        0   \n",
       "3         1     48     15       2       0      0    0       0        0   \n",
       "4         1     39      6       2       0      1    0       0        0   \n",
       "...     ...    ...    ...     ...     ...    ...  ...     ...      ...   \n",
       "1951      0     58     12       2       0      0    0       0        0   \n",
       "1952      0     93     20       1       1      0    0       0        0   \n",
       "1953      0      3     13       0       0      0    0       0        0   \n",
       "1954      0     23     14       0       0      0    0       0        0   \n",
       "1955      0     26     11       0       0      0    0       0        0   \n",
       "\n",
       "      subscribe  channel  song  love  views  \n",
       "0             0        1     0     0      0  \n",
       "1             1        1     0     0      0  \n",
       "2             0        0     0     0      0  \n",
       "3             0        1     0     0      0  \n",
       "4             0        0     0     0      0  \n",
       "...         ...      ...   ...   ...    ...  \n",
       "1951          0        0     1     1      0  \n",
       "1952          0        0     1     1      0  \n",
       "1953          0        0     0     0      0  \n",
       "1954          0        0     0     0      0  \n",
       "1955          0        0     0     0      0  \n",
       "\n",
       "[1956 rows x 16 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFinal = dfAll\n",
    "\n",
    "dfFinal['check'] = check\n",
    "dfFinal['com'] = com\n",
    "dfFinal['please'] = please\n",
    "dfFinal['youtube'] = youtube\n",
    "dfFinal['subscribe'] = subscribe\n",
    "dfFinal['channel'] = channel\n",
    "dfFinal['song'] = song\n",
    "dfFinal['love'] = love\n",
    "dfFinal['views'] = views\n",
    "dfFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5aa25c30-e424-4afd-9613-1729b9aa1a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Preparing variables for training\n",
    "X=dfFinal.drop(columns = ['AUTHOR','CONTENT','CLASS'])\n",
    "y=dfFinal['CLASS']\n",
    "\n",
    "#Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0fe195cd-0d55-4a58-8a99-27af719a3cc2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using kernel linear, C value is 0.01, the acc is 0.877\n",
      "[[271  30]\n",
      " [ 42 244]]\n",
      "using kernel linear, C value is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 0.5, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "using kernel linear, C value is 1, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "using kernel linear, C value is 5, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "using kernel linear, C value is 10, the acc is 0.908\n",
      "[[280  21]\n",
      " [ 33 253]]\n",
      "using kernel rbf, C value is 0.01, the acc is 0.627\n",
      "[[254  47]\n",
      " [172 114]]\n",
      "using kernel rbf, C value is 0.1, the acc is 0.605\n",
      "[[219  82]\n",
      " [150 136]]\n",
      "using kernel rbf, C value is 0.5, the acc is 0.617\n",
      "[[189 112]\n",
      " [113 173]]\n",
      "using kernel rbf, C value is 1, the acc is 0.618\n",
      "[[185 116]\n",
      " [108 178]]\n",
      "using kernel rbf, C value is 5, the acc is 0.618\n",
      "[[168 133]\n",
      " [ 91 195]]\n",
      "using kernel rbf, C value is 10, the acc is 0.646\n",
      "[[165 136]\n",
      " [ 72 214]]\n",
      "using kernel poly, C value is 0.01, the acc is 0.596\n",
      "[[296   5]\n",
      " [232  54]]\n",
      "using kernel poly, C value is 0.1, the acc is 0.600\n",
      "[[294   7]\n",
      " [228  58]]\n",
      "using kernel poly, C value is 0.5, the acc is 0.606\n",
      "[[292   9]\n",
      " [222  64]]\n",
      "using kernel poly, C value is 1, the acc is 0.624\n",
      "[[292   9]\n",
      " [212  74]]\n",
      "using kernel poly, C value is 5, the acc is 0.630\n",
      "[[289  12]\n",
      " [205  81]]\n",
      "using kernel poly, C value is 10, the acc is 0.632\n",
      "[[290  11]\n",
      " [205  81]]\n"
     ]
    }
   ],
   "source": [
    "#SVC: Testing several kernal types and C values\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.01, 0.1, 0.5, 1, 5, 10]\n",
    "for kernel in kernels: \n",
    "    for c_val in C_values: \n",
    "        model = SVC(kernel=kernel, C=c_val, probability = True, random_state=10)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        print(\"using kernel {}\".format(kernel) + \", C value is {}\".format(c_val) +\n",
    "              \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "        print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "03904569-7e59-43f7-a86d-d84e9bab564c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss hinge,  penalty is l1,  alpha is 0.001, the acc is 0.894\n",
      "[[273  28]\n",
      " [ 34 252]]\n",
      "0.8943781942078365\n",
      "using loss hinge,  penalty is l1,  alpha is 0.1, the acc is 0.637\n",
      "[[202  99]\n",
      " [114 172]]\n",
      "0.637137989778535\n",
      "using loss hinge,  penalty is l1,  alpha is 1, the acc is 0.620\n",
      "[[193 108]\n",
      " [115 171]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is l1,  alpha is 5, the acc is 0.620\n",
      "[[184 117]\n",
      " [106 180]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is l1,  alpha is 10, the acc is 0.613\n",
      "[[217  84]\n",
      " [143 143]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is l2,  alpha is 0.001, the acc is 0.780\n",
      "[[220  81]\n",
      " [ 48 238]]\n",
      "0.7802385008517888\n",
      "using loss hinge,  penalty is l2,  alpha is 0.1, the acc is 0.736\n",
      "[[199 102]\n",
      " [ 53 233]]\n",
      "0.7359454855195912\n",
      "using loss hinge,  penalty is l2,  alpha is 1, the acc is 0.634\n",
      "[[211  90]\n",
      " [125 161]]\n",
      "0.6337308347529813\n",
      "using loss hinge,  penalty is l2,  alpha is 5, the acc is 0.612\n",
      "[[202  99]\n",
      " [129 157]]\n",
      "0.6115843270868825\n",
      "using loss hinge,  penalty is l2,  alpha is 10, the acc is 0.620\n",
      "[[227  74]\n",
      " [149 137]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is None,  alpha is 0.001, the acc is 0.867\n",
      "[[241  60]\n",
      " [ 18 268]]\n",
      "0.8671209540034072\n",
      "using loss hinge,  penalty is None,  alpha is 0.1, the acc is 0.826\n",
      "[[283  18]\n",
      " [ 84 202]]\n",
      "0.8262350936967632\n",
      "using loss hinge,  penalty is None,  alpha is 1, the acc is 0.828\n",
      "[[258  43]\n",
      " [ 58 228]]\n",
      "0.82793867120954\n",
      "using loss hinge,  penalty is None,  alpha is 5, the acc is 0.646\n",
      "[[231  70]\n",
      " [138 148]]\n",
      "0.645655877342419\n",
      "using loss hinge,  penalty is None,  alpha is 10, the acc is 0.610\n",
      "[[227  74]\n",
      " [155 131]]\n",
      "0.6098807495741057\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.001, the acc is 0.802\n",
      "[[195 106]\n",
      " [ 10 276]]\n",
      "0.8023850085178875\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.1, the acc is 0.625\n",
      "[[196 105]\n",
      " [115 171]]\n",
      "0.6252129471890971\n",
      "using loss log_loss,  penalty is l1,  alpha is 1, the acc is 0.624\n",
      "[[280  21]\n",
      " [200  86]]\n",
      "0.6235093696763203\n",
      "using loss log_loss,  penalty is l1,  alpha is 5, the acc is 0.613\n",
      "[[166 135]\n",
      " [ 92 194]]\n",
      "0.6132879045996593\n",
      "using loss log_loss,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.001, the acc is 0.770\n",
      "[[200 101]\n",
      " [ 34 252]]\n",
      "0.7700170357751278\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.1, the acc is 0.702\n",
      "[[207  94]\n",
      " [ 81 205]]\n",
      "0.7018739352640545\n",
      "using loss log_loss,  penalty is l2,  alpha is 1, the acc is 0.651\n",
      "[[281  20]\n",
      " [185 101]]\n",
      "0.6507666098807495\n",
      "using loss log_loss,  penalty is l2,  alpha is 5, the acc is 0.606\n",
      "[[178 123]\n",
      " [108 178]]\n",
      "0.606473594548552\n",
      "using loss log_loss,  penalty is l2,  alpha is 10, the acc is 0.606\n",
      "[[182 119]\n",
      " [112 174]]\n",
      "0.606473594548552\n",
      "using loss log_loss,  penalty is None,  alpha is 0.001, the acc is 0.901\n",
      "[[268  33]\n",
      " [ 25 261]]\n",
      "0.9011925042589438\n",
      "using loss log_loss,  penalty is None,  alpha is 0.1, the acc is 0.894\n",
      "[[267  34]\n",
      " [ 28 258]]\n",
      "0.8943781942078365\n",
      "using loss log_loss,  penalty is None,  alpha is 1, the acc is 0.816\n",
      "[[275  26]\n",
      " [ 82 204]]\n",
      "0.8160136286201022\n",
      "using loss log_loss,  penalty is None,  alpha is 5, the acc is 0.719\n",
      "[[234  67]\n",
      " [ 98 188]]\n",
      "0.7189097103918228\n",
      "using loss log_loss,  penalty is None,  alpha is 10, the acc is 0.630\n",
      "[[237  64]\n",
      " [153 133]]\n",
      "0.6303236797274276\n",
      "using loss log,  penalty is l1,  alpha is 0.001, the acc is 0.802\n",
      "[[195 106]\n",
      " [ 10 276]]\n",
      "0.8023850085178875\n",
      "using loss log,  penalty is l1,  alpha is 0.1, the acc is 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196 105]\n",
      " [115 171]]\n",
      "0.6252129471890971\n",
      "using loss log,  penalty is l1,  alpha is 1, the acc is 0.624\n",
      "[[280  21]\n",
      " [200  86]]\n",
      "0.6235093696763203\n",
      "using loss log,  penalty is l1,  alpha is 5, the acc is 0.613\n",
      "[[166 135]\n",
      " [ 92 194]]\n",
      "0.6132879045996593\n",
      "using loss log,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l2,  alpha is 0.001, the acc is 0.770\n",
      "[[200 101]\n",
      " [ 34 252]]\n",
      "0.7700170357751278\n",
      "using loss log,  penalty is l2,  alpha is 0.1, the acc is 0.702\n",
      "[[207  94]\n",
      " [ 81 205]]\n",
      "0.7018739352640545\n",
      "using loss log,  penalty is l2,  alpha is 1, the acc is 0.651\n",
      "[[281  20]\n",
      " [185 101]]\n",
      "0.6507666098807495\n",
      "using loss log,  penalty is l2,  alpha is 5, the acc is 0.606\n",
      "[[178 123]\n",
      " [108 178]]\n",
      "0.606473594548552\n",
      "using loss log,  penalty is l2,  alpha is 10, the acc is 0.606\n",
      "[[182 119]\n",
      " [112 174]]\n",
      "0.606473594548552\n",
      "using loss log,  penalty is None,  alpha is 0.001, the acc is 0.901\n",
      "[[268  33]\n",
      " [ 25 261]]\n",
      "0.9011925042589438\n",
      "using loss log,  penalty is None,  alpha is 0.1, the acc is 0.894\n",
      "[[267  34]\n",
      " [ 28 258]]\n",
      "0.8943781942078365\n",
      "using loss log,  penalty is None,  alpha is 1, the acc is 0.816\n",
      "[[275  26]\n",
      " [ 82 204]]\n",
      "0.8160136286201022\n",
      "using loss log,  penalty is None,  alpha is 5, the acc is 0.719\n",
      "[[234  67]\n",
      " [ 98 188]]\n",
      "0.7189097103918228\n",
      "using loss log,  penalty is None,  alpha is 10, the acc is 0.630\n",
      "[[237  64]\n",
      " [153 133]]\n",
      "0.6303236797274276\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.001, the acc is 0.898\n",
      "[[261  40]\n",
      " [ 20 266]]\n",
      "0.8977853492333902\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.1, the acc is 0.796\n",
      "[[202  99]\n",
      " [ 21 265]]\n",
      "0.7955706984667802\n",
      "using loss modified_huber,  penalty is l1,  alpha is 1, the acc is 0.606\n",
      "[[217  84]\n",
      " [147 139]]\n",
      "0.606473594548552\n",
      "using loss modified_huber,  penalty is l1,  alpha is 5, the acc is 0.627\n",
      "[[248  53]\n",
      " [166 120]]\n",
      "0.626916524701874\n",
      "using loss modified_huber,  penalty is l1,  alpha is 10, the acc is 0.625\n",
      "[[283  18]\n",
      " [202  84]]\n",
      "0.6252129471890971\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.001, the acc is 0.782\n",
      "[[233  68]\n",
      " [ 60 226]]\n",
      "0.7819420783645656\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.1, the acc is 0.705\n",
      "[[181 120]\n",
      " [ 53 233]]\n",
      "0.7052810902896082\n",
      "using loss modified_huber,  penalty is l2,  alpha is 1, the acc is 0.666\n",
      "[[196 105]\n",
      " [ 91 195]]\n",
      "0.666098807495741\n",
      "using loss modified_huber,  penalty is l2,  alpha is 5, the acc is 0.639\n",
      "[[228  73]\n",
      " [139 147]]\n",
      "0.6388415672913118\n",
      "using loss modified_huber,  penalty is l2,  alpha is 10, the acc is 0.647\n",
      "[[270  31]\n",
      " [176 110]]\n",
      "0.6473594548551959\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.001, the acc is 0.884\n",
      "[[282  19]\n",
      " [ 49 237]]\n",
      "0.8841567291311755\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.1, the acc is 0.881\n",
      "[[280  21]\n",
      " [ 49 237]]\n",
      "0.8807495741056218\n",
      "using loss modified_huber,  penalty is None,  alpha is 1, the acc is 0.862\n",
      "[[249  52]\n",
      " [ 29 257]]\n",
      "0.8620102214650767\n",
      "using loss modified_huber,  penalty is None,  alpha is 5, the acc is 0.845\n",
      "[[271  30]\n",
      " [ 61 225]]\n",
      "0.8449744463373083\n",
      "using loss modified_huber,  penalty is None,  alpha is 10, the acc is 0.809\n",
      "[[242  59]\n",
      " [ 53 233]]\n",
      "0.8091993185689949\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.001, the acc is 0.888\n",
      "[[263  38]\n",
      " [ 28 258]]\n",
      "0.8875638841567292\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.1, the acc is 0.899\n",
      "[[278  23]\n",
      " [ 36 250]]\n",
      "0.899488926746167\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 1, the acc is 0.893\n",
      "[[272  29]\n",
      " [ 34 252]]\n",
      "0.8926746166950597\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 5, the acc is 0.876\n",
      "[[249  52]\n",
      " [ 21 265]]\n",
      "0.8756388415672913\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 10, the acc is 0.879\n",
      "[[275  26]\n",
      " [ 45 241]]\n",
      "0.879045996592845\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.001, the acc is 0.785\n",
      "[[228  73]\n",
      " [ 53 233]]\n",
      "0.7853492333901193\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.1, the acc is 0.702\n",
      "[[196 105]\n",
      " [ 70 216]]\n",
      "0.7018739352640545\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 1, the acc is 0.635\n",
      "[[216  85]\n",
      " [129 157]]\n",
      "0.6354344122657581\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 5, the acc is 0.642\n",
      "[[259  42]\n",
      " [168 118]]\n",
      "0.6422487223168655\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 10, the acc is 0.630\n",
      "[[253  48]\n",
      " [169 117]]\n",
      "0.6303236797274276\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.001, the acc is 0.888\n",
      "[[256  45]\n",
      " [ 21 265]]\n",
      "0.8875638841567292\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.1, the acc is 0.816\n",
      "[[288  13]\n",
      " [ 95 191]]\n",
      "0.8160136286201022\n",
      "using loss squared_hinge,  penalty is None,  alpha is 1, the acc is 0.891\n",
      "[[272  29]\n",
      " [ 35 251]]\n",
      "0.8909710391822828\n",
      "using loss squared_hinge,  penalty is None,  alpha is 5, the acc is 0.857\n",
      "[[251  50]\n",
      " [ 34 252]]\n",
      "0.8568994889267462\n",
      "using loss squared_hinge,  penalty is None,  alpha is 10, the acc is 0.843\n",
      "[[277  24]\n",
      " [ 68 218]]\n",
      "0.8432708688245315\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.001, the acc is 0.886\n",
      "[[286  15]\n",
      " [ 52 234]]\n",
      "0.8858603066439523\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss perceptron,  penalty is l1,  alpha is 1, the acc is 0.625\n",
      "[[274  27]\n",
      " [193  93]]\n",
      "0.6252129471890971\n",
      "using loss perceptron,  penalty is l1,  alpha is 5, the acc is 0.629\n",
      "[[250  51]\n",
      " [167 119]]\n",
      "0.6286201022146508\n",
      "using loss perceptron,  penalty is l1,  alpha is 10, the acc is 0.627\n",
      "[[255  46]\n",
      " [173 113]]\n",
      "0.626916524701874\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.001, the acc is 0.644\n",
      "[[296   5]\n",
      " [204  82]]\n",
      "0.6439522998296422\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.1, the acc is 0.702\n",
      "[[207  94]\n",
      " [ 81 205]]\n",
      "0.7018739352640545\n",
      "using loss perceptron,  penalty is l2,  alpha is 1, the acc is 0.652\n",
      "[[179 122]\n",
      " [ 82 204]]\n",
      "0.6524701873935264\n",
      "using loss perceptron,  penalty is l2,  alpha is 5, the acc is 0.632\n",
      "[[248  53]\n",
      " [163 123]]\n",
      "0.6320272572402045\n",
      "using loss perceptron,  penalty is l2,  alpha is 10, the acc is 0.618\n",
      "[[173 128]\n",
      " [ 96 190]]\n",
      "0.6183986371379898\n",
      "using loss perceptron,  penalty is None,  alpha is 0.001, the acc is 0.744\n",
      "[[296   5]\n",
      " [145 141]]\n",
      "0.7444633730834753\n",
      "using loss perceptron,  penalty is None,  alpha is 0.1, the acc is 0.830\n",
      "[[221  80]\n",
      " [ 20 266]]\n",
      "0.8296422487223168\n",
      "using loss perceptron,  penalty is None,  alpha is 1, the acc is 0.860\n",
      "[[277  24]\n",
      " [ 58 228]]\n",
      "0.8603066439522998\n",
      "using loss perceptron,  penalty is None,  alpha is 5, the acc is 0.663\n",
      "[[241  60]\n",
      " [138 148]]\n",
      "0.6626916524701874\n",
      "using loss perceptron,  penalty is None,  alpha is 10, the acc is 0.610\n",
      "[[227  74]\n",
      " [155 131]]\n",
      "0.6098807495741057\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.001, the acc is 0.489\n",
      "[[286  15]\n",
      " [285   1]]\n",
      "0.4889267461669506\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.1, the acc is 0.445\n",
      "[[251  50]\n",
      " [276  10]]\n",
      "0.444633730834753\n",
      "using loss squared_error,  penalty is l1,  alpha is 1, the acc is 0.542\n",
      "[[ 79 222]\n",
      " [ 47 239]]\n",
      "0.5417376490630323\n",
      "using loss squared_error,  penalty is l1,  alpha is 5, the acc is 0.666\n",
      "[[186 115]\n",
      " [ 81 205]]\n",
      "0.666098807495741\n",
      "using loss squared_error,  penalty is l1,  alpha is 10, the acc is 0.724\n",
      "[[279  22]\n",
      " [140 146]]\n",
      "0.7240204429301533\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.001, the acc is 0.506\n",
      "[[297   4]\n",
      " [286   0]]\n",
      "0.5059625212947189\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.1, the acc is 0.503\n",
      "[[291  10]\n",
      " [282   4]]\n",
      "0.5025553662691652\n",
      "using loss squared_error,  penalty is l2,  alpha is 1, the acc is 0.489\n",
      "[[ 59 242]\n",
      " [ 58 228]]\n",
      "0.4889267461669506\n",
      "using loss squared_error,  penalty is l2,  alpha is 5, the acc is 0.612\n",
      "[[249  52]\n",
      " [176 110]]\n",
      "0.6115843270868825\n",
      "using loss squared_error,  penalty is l2,  alpha is 10, the acc is 0.453\n",
      "[[ 43 258]\n",
      " [ 63 223]]\n",
      "0.45315161839863716\n",
      "using loss squared_error,  penalty is None,  alpha is 0.001, the acc is 0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[293   8]\n",
      " [284   2]]\n",
      "0.5025553662691652\n",
      "using loss squared_error,  penalty is None,  alpha is 0.1, the acc is 0.492\n",
      "[[  6 295]\n",
      " [  3 283]]\n",
      "0.49233390119250425\n",
      "using loss squared_error,  penalty is None,  alpha is 1, the acc is 0.629\n",
      "[[155 146]\n",
      " [ 72 214]]\n",
      "0.6286201022146508\n",
      "using loss squared_error,  penalty is None,  alpha is 5, the acc is 0.652\n",
      "[[205  96]\n",
      " [108 178]]\n",
      "0.6524701873935264\n",
      "using loss squared_error,  penalty is None,  alpha is 10, the acc is 0.734\n",
      "[[283  18]\n",
      " [138 148]]\n",
      "0.7342419080068143\n",
      "using loss huber,  penalty is l1,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 0.1, the acc is 0.581\n",
      "[[299   2]\n",
      " [244  42]]\n",
      "0.5809199318568995\n",
      "using loss huber,  penalty is l1,  alpha is 1, the acc is 0.613\n",
      "[[171 130]\n",
      " [ 97 189]]\n",
      "0.6132879045996593\n",
      "using loss huber,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 0.1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 1, the acc is 0.625\n",
      "[[281  20]\n",
      " [200  86]]\n",
      "0.6252129471890971\n",
      "using loss huber,  penalty is l2,  alpha is 5, the acc is 0.603\n",
      "[[200 101]\n",
      " [132 154]]\n",
      "0.6030664395229983\n",
      "using loss huber,  penalty is l2,  alpha is 10, the acc is 0.608\n",
      "[[165 136]\n",
      " [ 94 192]]\n",
      "0.6081771720613288\n",
      "using loss huber,  penalty is None,  alpha is 0.001, the acc is 0.671\n",
      "[[297   4]\n",
      " [189  97]]\n",
      "0.6712095400340715\n",
      "using loss huber,  penalty is None,  alpha is 0.1, the acc is 0.792\n",
      "[[274  27]\n",
      " [ 95 191]]\n",
      "0.7921635434412265\n",
      "using loss huber,  penalty is None,  alpha is 1, the acc is 0.644\n",
      "[[289  12]\n",
      " [197  89]]\n",
      "0.6439522998296422\n",
      "using loss huber,  penalty is None,  alpha is 5, the acc is 0.624\n",
      "[[261  40]\n",
      " [181 105]]\n",
      "0.6235093696763203\n",
      "using loss huber,  penalty is None,  alpha is 10, the acc is 0.612\n",
      "[[279  22]\n",
      " [206  80]]\n",
      "0.6115843270868825\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.898\n",
      "[[275  26]\n",
      " [ 34 252]]\n",
      "0.8977853492333902\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.554\n",
      "[[299   2]\n",
      " [260  26]]\n",
      "0.5536626916524702\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.624\n",
      "[[285  16]\n",
      " [205  81]]\n",
      "0.6235093696763203\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.627\n",
      "[[253  48]\n",
      " [171 115]]\n",
      "0.626916524701874\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.487\n",
      "[[  0 301]\n",
      " [  0 286]]\n",
      "0.48722316865417375\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.704\n",
      "[[291  10]\n",
      " [164 122]]\n",
      "0.7035775127768313\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.598\n",
      "[[298   3]\n",
      " [233  53]]\n",
      "0.5979557069846678\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.627\n",
      "[[277  24]\n",
      " [195  91]]\n",
      "0.626916524701874\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.603\n",
      "[[294   7]\n",
      " [226  60]]\n",
      "0.6030664395229983\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.497\n",
      "[[292   9]\n",
      " [286   0]]\n",
      "0.49744463373083475\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.654\n",
      "[[280  21]\n",
      " [182 104]]\n",
      "0.6541737649063032\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.668\n",
      "[[288  13]\n",
      " [182 104]]\n",
      "0.6678023850085179\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.760\n",
      "[[203  98]\n",
      " [ 43 243]]\n",
      "0.7597955706984668\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.756\n",
      "[[228  73]\n",
      " [ 70 216]]\n",
      "0.7563884156729132\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.487\n",
      "[[  4 297]\n",
      " [  4 282]]\n",
      "0.48722316865417375\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.470\n",
      "[[144 157]\n",
      " [154 132]]\n",
      "0.47018739352640543\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.734\n",
      "[[255  46]\n",
      " [110 176]]\n",
      "0.7342419080068143\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.610\n",
      "[[226  75]\n",
      " [154 132]]\n",
      "0.6098807495741057\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.499\n",
      "[[292   9]\n",
      " [285   1]]\n",
      "0.4991482112436116\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.497\n",
      "[[290  11]\n",
      " [284   2]]\n",
      "0.49744463373083475\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.513\n",
      "[[201 100]\n",
      " [186 100]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.370\n",
      "[[ 55 246]\n",
      " [124 162]]\n",
      "0.3696763202725724\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.395\n",
      "[[ 42 259]\n",
      " [ 96 190]]\n",
      "0.39522998296422485\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.578\n",
      "[[180 121]\n",
      " [127 159]]\n",
      "0.5775127768313458\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.467\n",
      "[[181 120]\n",
      " [193  93]]\n",
      "0.46678023850085176\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.760\n",
      "[[292   9]\n",
      " [132 154]]\n",
      "0.7597955706984668\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.506\n",
      "[[115 186]\n",
      " [104 182]]\n",
      "0.5059625212947189\n"
     ]
    }
   ],
   "source": [
    "#SGDClassification\n",
    "X=dfFinal.drop(columns = ['AUTHOR','CONTENT','CLASS'])\n",
    "y=dfFinal['CLASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "losses = ['hinge', 'log_loss', 'log', 'modified_huber', \n",
    "          'squared_hinge', 'perceptron', 'squared_error',\n",
    "          'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalties = ['l1','l2',None]\n",
    "alphas = [0.001, .1, 1, 5, 10]\n",
    "for _loss in losses: \n",
    "    for _penalty in penalties:\n",
    "        for _alpha in alphas:\n",
    "            model = SGDClassifier(loss=_loss, penalty=_penalty, alpha=_alpha, random_state=10)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            print(\"using loss {}\".format(_loss) + \",  penalty is {}\".format(_penalty) +\n",
    "                  \",  alpha is {}\".format(_alpha) + \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "            print(confusion_matrix(y_test, pred))\n",
    "            print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "35ac46f9-efe7-453c-9b20-d33a16d0e4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[259  42]\n",
      " [ 25 261]]\n",
      "0.8858603066439523\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier:\n",
    "model = DecisionTreeClassifier(random_state = 10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "05952d64-2deb-49cc-98ab-379c29b06885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[280  21]\n",
      " [ 33 253]]\n",
      "0.9080068143100511\n"
     ]
    }
   ],
   "source": [
    "#SVC with kernel=linear and C=1 had highest acc\n",
    "model = SVC(kernel='linear', C=1, probability = True, random_state=10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "da50ad7b-9be6-43f5-ab09-73afbba8f41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code taken from https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "#Generating ROC and AUC for Linear & C=1\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC / AUC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "298b98e5-b9b7-420c-b861-658bcd1363df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) C_LEN                                     0.218954\n",
      " 2) check                                     0.196641\n",
      " 3) C_SPEC                                    0.127033\n",
      " 4) subscribe                                 0.120458\n",
      " 5) com                                       0.089619\n",
      " 6) A_LEN                                     0.067099\n",
      " 7) channel                                   0.052361\n",
      " 8) please                                    0.046266\n",
      " 9) youtube                                   0.030081\n",
      "10) song                                      0.021453\n",
      "11) views                                     0.017116\n",
      "12) love                                      0.011446\n",
      "13) A_SPEC                                    0.001475\n"
     ]
    }
   ],
   "source": [
    "#Testing importance of each attribute\n",
    "feat_labels = X.columns[:]\n",
    "\n",
    "forest = RandomForestClassifier(random_state = 10)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    if f < 15:\n",
    "        print(\"%2d) %-*s %f\" % (f + 1, 41, feat_labels[indices[f]], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8a7ee93d-435f-4ed1-95d0-2e0e61de90dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Preparing variables for training\n",
    "X=dfFinal.drop(columns = ['AUTHOR','CONTENT','CLASS','A_SPEC','love','views','song'])\n",
    "y=dfFinal['CLASS']\n",
    "\n",
    "#Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5d4148f9-a7d1-42c4-b2de-c4d35202ab5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using kernel linear, C value is 0.01, the acc is 0.877\n",
      "[[274  27]\n",
      " [ 45 241]]\n",
      "using kernel linear, C value is 0.1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 0.5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 1, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 5, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel linear, C value is 10, the acc is 0.901\n",
      "[[274  27]\n",
      " [ 31 255]]\n",
      "using kernel rbf, C value is 0.01, the acc is 0.627\n",
      "[[254  47]\n",
      " [172 114]]\n",
      "using kernel rbf, C value is 0.1, the acc is 0.605\n",
      "[[219  82]\n",
      " [150 136]]\n",
      "using kernel rbf, C value is 0.5, the acc is 0.615\n",
      "[[188 113]\n",
      " [113 173]]\n",
      "using kernel rbf, C value is 1, the acc is 0.618\n",
      "[[185 116]\n",
      " [108 178]]\n",
      "using kernel rbf, C value is 5, the acc is 0.620\n",
      "[[169 132]\n",
      " [ 91 195]]\n",
      "using kernel rbf, C value is 10, the acc is 0.646\n",
      "[[165 136]\n",
      " [ 72 214]]\n",
      "using kernel poly, C value is 0.01, the acc is 0.596\n",
      "[[296   5]\n",
      " [232  54]]\n",
      "using kernel poly, C value is 0.1, the acc is 0.600\n",
      "[[294   7]\n",
      " [228  58]]\n",
      "using kernel poly, C value is 0.5, the acc is 0.606\n",
      "[[292   9]\n",
      " [222  64]]\n",
      "using kernel poly, C value is 1, the acc is 0.624\n",
      "[[292   9]\n",
      " [212  74]]\n",
      "using kernel poly, C value is 5, the acc is 0.630\n",
      "[[288  13]\n",
      " [204  82]]\n",
      "using kernel poly, C value is 10, the acc is 0.630\n",
      "[[289  12]\n",
      " [205  81]]\n"
     ]
    }
   ],
   "source": [
    "#SVC: Testing several kernal types and C values\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.01, 0.1, 0.5, 1, 5, 10]\n",
    "for kernel in kernels: \n",
    "    for c_val in C_values: \n",
    "        model = SVC(kernel=kernel, C=c_val, probability = True, random_state=10)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        print(\"using kernel {}\".format(kernel) + \", C value is {}\".format(c_val) +\n",
    "              \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "        print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "57b340a6-3fbb-4156-a895-286eaf8f866f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss hinge,  penalty is l1,  alpha is 0.001, the acc is 0.891\n",
      "[[279  22]\n",
      " [ 42 244]]\n",
      "0.8909710391822828\n",
      "using loss hinge,  penalty is l1,  alpha is 0.1, the acc is 0.637\n",
      "[[202  99]\n",
      " [114 172]]\n",
      "0.637137989778535\n",
      "using loss hinge,  penalty is l1,  alpha is 1, the acc is 0.620\n",
      "[[193 108]\n",
      " [115 171]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is l1,  alpha is 5, the acc is 0.620\n",
      "[[184 117]\n",
      " [106 180]]\n",
      "0.6201022146507666\n",
      "using loss hinge,  penalty is l1,  alpha is 10, the acc is 0.613\n",
      "[[217  84]\n",
      " [143 143]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is l2,  alpha is 0.001, the acc is 0.748\n",
      "[[209  92]\n",
      " [ 56 230]]\n",
      "0.747870528109029\n",
      "using loss hinge,  penalty is l2,  alpha is 0.1, the acc is 0.734\n",
      "[[219  82]\n",
      " [ 74 212]]\n",
      "0.7342419080068143\n",
      "using loss hinge,  penalty is l2,  alpha is 1, the acc is 0.639\n",
      "[[203  98]\n",
      " [114 172]]\n",
      "0.6388415672913118\n",
      "using loss hinge,  penalty is l2,  alpha is 5, the acc is 0.613\n",
      "[[198 103]\n",
      " [124 162]]\n",
      "0.6132879045996593\n",
      "using loss hinge,  penalty is l2,  alpha is 10, the acc is 0.618\n",
      "[[230  71]\n",
      " [153 133]]\n",
      "0.6183986371379898\n",
      "using loss hinge,  penalty is None,  alpha is 0.001, the acc is 0.888\n",
      "[[281  20]\n",
      " [ 46 240]]\n",
      "0.8875638841567292\n",
      "using loss hinge,  penalty is None,  alpha is 0.1, the acc is 0.882\n",
      "[[277  24]\n",
      " [ 45 241]]\n",
      "0.8824531516183987\n",
      "using loss hinge,  penalty is None,  alpha is 1, the acc is 0.833\n",
      "[[263  38]\n",
      " [ 60 226]]\n",
      "0.8330494037478705\n",
      "using loss hinge,  penalty is None,  alpha is 5, the acc is 0.641\n",
      "[[235  66]\n",
      " [145 141]]\n",
      "0.6405451448040886\n",
      "using loss hinge,  penalty is None,  alpha is 10, the acc is 0.608\n",
      "[[226  75]\n",
      " [155 131]]\n",
      "0.6081771720613288\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.001, the acc is 0.503\n",
      "[[  9 292]\n",
      " [  0 286]]\n",
      "0.5025553662691652\n",
      "using loss log_loss,  penalty is l1,  alpha is 0.1, the acc is 0.625\n",
      "[[196 105]\n",
      " [115 171]]\n",
      "0.6252129471890971\n",
      "using loss log_loss,  penalty is l1,  alpha is 1, the acc is 0.624\n",
      "[[280  21]\n",
      " [200  86]]\n",
      "0.6235093696763203\n",
      "using loss log_loss,  penalty is l1,  alpha is 5, the acc is 0.613\n",
      "[[166 135]\n",
      " [ 92 194]]\n",
      "0.6132879045996593\n",
      "using loss log_loss,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.001, the acc is 0.746\n",
      "[[207  94]\n",
      " [ 55 231]]\n",
      "0.7461669505962522\n",
      "using loss log_loss,  penalty is l2,  alpha is 0.1, the acc is 0.688\n",
      "[[196 105]\n",
      " [ 78 208]]\n",
      "0.6882453151618398\n",
      "using loss log_loss,  penalty is l2,  alpha is 1, the acc is 0.646\n",
      "[[281  20]\n",
      " [188  98]]\n",
      "0.645655877342419\n",
      "using loss log_loss,  penalty is l2,  alpha is 5, the acc is 0.605\n",
      "[[178 123]\n",
      " [109 177]]\n",
      "0.6047700170357752\n",
      "using loss log_loss,  penalty is l2,  alpha is 10, the acc is 0.606\n",
      "[[182 119]\n",
      " [112 174]]\n",
      "0.606473594548552\n",
      "using loss log_loss,  penalty is None,  alpha is 0.001, the acc is 0.874\n",
      "[[245  56]\n",
      " [ 18 268]]\n",
      "0.8739352640545145\n",
      "using loss log_loss,  penalty is None,  alpha is 0.1, the acc is 0.881\n",
      "[[279  22]\n",
      " [ 48 238]]\n",
      "0.8807495741056218\n",
      "using loss log_loss,  penalty is None,  alpha is 1, the acc is 0.811\n",
      "[[275  26]\n",
      " [ 85 201]]\n",
      "0.8109028960817717\n",
      "using loss log_loss,  penalty is None,  alpha is 5, the acc is 0.697\n",
      "[[232  69]\n",
      " [109 177]]\n",
      "0.696763202725724\n",
      "using loss log_loss,  penalty is None,  alpha is 10, the acc is 0.627\n",
      "[[236  65]\n",
      " [154 132]]\n",
      "0.626916524701874\n",
      "using loss log,  penalty is l1,  alpha is 0.001, the acc is 0.503\n",
      "[[  9 292]\n",
      " [  0 286]]\n",
      "0.5025553662691652\n",
      "using loss log,  penalty is l1,  alpha is 0.1, the acc is 0.625\n",
      "[[196 105]\n",
      " [115 171]]\n",
      "0.6252129471890971\n",
      "using loss log,  penalty is l1,  alpha is 1, the acc is 0.624\n",
      "[[280  21]\n",
      " [200  86]]\n",
      "0.6235093696763203\n",
      "using loss log,  penalty is l1,  alpha is 5, the acc is 0.613\n",
      "[[166 135]\n",
      " [ 92 194]]\n",
      "0.6132879045996593\n",
      "using loss log,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss log,  penalty is l2,  alpha is 0.001, the acc is 0.746\n",
      "[[207  94]\n",
      " [ 55 231]]\n",
      "0.7461669505962522\n",
      "using loss log,  penalty is l2,  alpha is 0.1, the acc is 0.688\n",
      "[[196 105]\n",
      " [ 78 208]]\n",
      "0.6882453151618398\n",
      "using loss log,  penalty is l2,  alpha is 1, the acc is 0.646\n",
      "[[281  20]\n",
      " [188  98]]\n",
      "0.645655877342419\n",
      "using loss log,  penalty is l2,  alpha is 5, the acc is 0.605\n",
      "[[178 123]\n",
      " [109 177]]\n",
      "0.6047700170357752\n",
      "using loss log,  penalty is l2,  alpha is 10, the acc is 0.606\n",
      "[[182 119]\n",
      " [112 174]]\n",
      "0.606473594548552\n",
      "using loss log,  penalty is None,  alpha is 0.001, the acc is 0.874\n",
      "[[245  56]\n",
      " [ 18 268]]\n",
      "0.8739352640545145\n",
      "using loss log,  penalty is None,  alpha is 0.1, the acc is 0.881\n",
      "[[279  22]\n",
      " [ 48 238]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8807495741056218\n",
      "using loss log,  penalty is None,  alpha is 1, the acc is 0.811\n",
      "[[275  26]\n",
      " [ 85 201]]\n",
      "0.8109028960817717\n",
      "using loss log,  penalty is None,  alpha is 5, the acc is 0.697\n",
      "[[232  69]\n",
      " [109 177]]\n",
      "0.696763202725724\n",
      "using loss log,  penalty is None,  alpha is 10, the acc is 0.627\n",
      "[[236  65]\n",
      " [154 132]]\n",
      "0.626916524701874\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.001, the acc is 0.891\n",
      "[[275  26]\n",
      " [ 38 248]]\n",
      "0.8909710391822828\n",
      "using loss modified_huber,  penalty is l1,  alpha is 0.1, the acc is 0.807\n",
      "[[208  93]\n",
      " [ 20 266]]\n",
      "0.807495741056218\n",
      "using loss modified_huber,  penalty is l1,  alpha is 1, the acc is 0.606\n",
      "[[217  84]\n",
      " [147 139]]\n",
      "0.606473594548552\n",
      "using loss modified_huber,  penalty is l1,  alpha is 5, the acc is 0.627\n",
      "[[248  53]\n",
      " [166 120]]\n",
      "0.626916524701874\n",
      "using loss modified_huber,  penalty is l1,  alpha is 10, the acc is 0.625\n",
      "[[283  18]\n",
      " [202  84]]\n",
      "0.6252129471890971\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.001, the acc is 0.525\n",
      "[[300   1]\n",
      " [278   8]]\n",
      "0.524701873935264\n",
      "using loss modified_huber,  penalty is l2,  alpha is 0.1, the acc is 0.702\n",
      "[[213  88]\n",
      " [ 87 199]]\n",
      "0.7018739352640545\n",
      "using loss modified_huber,  penalty is l2,  alpha is 1, the acc is 0.656\n",
      "[[190 111]\n",
      " [ 91 195]]\n",
      "0.65587734241908\n",
      "using loss modified_huber,  penalty is l2,  alpha is 5, the acc is 0.641\n",
      "[[228  73]\n",
      " [138 148]]\n",
      "0.6405451448040886\n",
      "using loss modified_huber,  penalty is l2,  alpha is 10, the acc is 0.646\n",
      "[[269  32]\n",
      " [176 110]]\n",
      "0.645655877342419\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.001, the acc is 0.867\n",
      "[[246  55]\n",
      " [ 23 263]]\n",
      "0.8671209540034072\n",
      "using loss modified_huber,  penalty is None,  alpha is 0.1, the acc is 0.893\n",
      "[[267  34]\n",
      " [ 29 257]]\n",
      "0.8926746166950597\n",
      "using loss modified_huber,  penalty is None,  alpha is 1, the acc is 0.884\n",
      "[[270  31]\n",
      " [ 37 249]]\n",
      "0.8841567291311755\n",
      "using loss modified_huber,  penalty is None,  alpha is 5, the acc is 0.848\n",
      "[[277  24]\n",
      " [ 65 221]]\n",
      "0.848381601362862\n",
      "using loss modified_huber,  penalty is None,  alpha is 10, the acc is 0.819\n",
      "[[263  38]\n",
      " [ 68 218]]\n",
      "0.8194207836456558\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.001, the acc is 0.886\n",
      "[[259  42]\n",
      " [ 25 261]]\n",
      "0.8858603066439523\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 0.1, the acc is 0.899\n",
      "[[276  25]\n",
      " [ 34 252]]\n",
      "0.899488926746167\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 1, the acc is 0.877\n",
      "[[279  22]\n",
      " [ 50 236]]\n",
      "0.8773424190800682\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 5, the acc is 0.888\n",
      "[[263  38]\n",
      " [ 28 258]]\n",
      "0.8875638841567292\n",
      "using loss squared_hinge,  penalty is l1,  alpha is 10, the acc is 0.559\n",
      "[[ 42 259]\n",
      " [  0 286]]\n",
      "0.5587734241908007\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.001, the acc is 0.770\n",
      "[[209  92]\n",
      " [ 43 243]]\n",
      "0.7700170357751278\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 0.1, the acc is 0.693\n",
      "[[186 115]\n",
      " [ 65 221]]\n",
      "0.6933560477001703\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 1, the acc is 0.634\n",
      "[[212  89]\n",
      " [126 160]]\n",
      "0.6337308347529813\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 5, the acc is 0.644\n",
      "[[259  42]\n",
      " [167 119]]\n",
      "0.6439522998296422\n",
      "using loss squared_hinge,  penalty is l2,  alpha is 10, the acc is 0.630\n",
      "[[253  48]\n",
      " [169 117]]\n",
      "0.6303236797274276\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.001, the acc is 0.874\n",
      "[[249  52]\n",
      " [ 22 264]]\n",
      "0.8739352640545145\n",
      "using loss squared_hinge,  penalty is None,  alpha is 0.1, the acc is 0.903\n",
      "[[272  29]\n",
      " [ 28 258]]\n",
      "0.9028960817717206\n",
      "using loss squared_hinge,  penalty is None,  alpha is 1, the acc is 0.867\n",
      "[[279  22]\n",
      " [ 56 230]]\n",
      "0.8671209540034072\n",
      "using loss squared_hinge,  penalty is None,  alpha is 5, the acc is 0.889\n",
      "[[263  38]\n",
      " [ 27 259]]\n",
      "0.889267461669506\n",
      "using loss squared_hinge,  penalty is None,  alpha is 10, the acc is 0.571\n",
      "[[ 50 251]\n",
      " [  1 285]]\n",
      "0.5706984667802385\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.001, the acc is 0.898\n",
      "[[286  15]\n",
      " [ 45 241]]\n",
      "0.8977853492333902\n",
      "using loss perceptron,  penalty is l1,  alpha is 0.1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss perceptron,  penalty is l1,  alpha is 1, the acc is 0.625\n",
      "[[274  27]\n",
      " [193  93]]\n",
      "0.6252129471890971\n",
      "using loss perceptron,  penalty is l1,  alpha is 5, the acc is 0.629\n",
      "[[250  51]\n",
      " [167 119]]\n",
      "0.6286201022146508\n",
      "using loss perceptron,  penalty is l1,  alpha is 10, the acc is 0.627\n",
      "[[255  46]\n",
      " [173 113]]\n",
      "0.626916524701874\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.001, the acc is 0.761\n",
      "[[208  93]\n",
      " [ 47 239]]\n",
      "0.7614991482112436\n",
      "using loss perceptron,  penalty is l2,  alpha is 0.1, the acc is 0.681\n",
      "[[162 139]\n",
      " [ 48 238]]\n",
      "0.6814310051107325\n",
      "using loss perceptron,  penalty is l2,  alpha is 1, the acc is 0.649\n",
      "[[192 109]\n",
      " [ 97 189]]\n",
      "0.6490630323679727\n",
      "using loss perceptron,  penalty is l2,  alpha is 5, the acc is 0.618\n",
      "[[213  88]\n",
      " [136 150]]\n",
      "0.6183986371379898\n",
      "using loss perceptron,  penalty is l2,  alpha is 10, the acc is 0.615\n",
      "[[172 129]\n",
      " [ 97 189]]\n",
      "0.6149914821124361\n",
      "using loss perceptron,  penalty is None,  alpha is 0.001, the acc is 0.879\n",
      "[[254  47]\n",
      " [ 24 262]]\n",
      "0.879045996592845\n",
      "using loss perceptron,  penalty is None,  alpha is 0.1, the acc is 0.891\n",
      "[[274  27]\n",
      " [ 37 249]]\n",
      "0.8909710391822828\n",
      "using loss perceptron,  penalty is None,  alpha is 1, the acc is 0.896\n",
      "[[270  31]\n",
      " [ 30 256]]\n",
      "0.8960817717206133\n",
      "using loss perceptron,  penalty is None,  alpha is 5, the acc is 0.652\n",
      "[[236  65]\n",
      " [139 147]]\n",
      "0.6524701873935264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loss perceptron,  penalty is None,  alpha is 10, the acc is 0.610\n",
      "[[226  75]\n",
      " [154 132]]\n",
      "0.6098807495741057\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.001, the acc is 0.499\n",
      "[[292   9]\n",
      " [285   1]]\n",
      "0.4991482112436116\n",
      "using loss squared_error,  penalty is l1,  alpha is 0.1, the acc is 0.499\n",
      "[[ 10 291]\n",
      " [  3 283]]\n",
      "0.4991482112436116\n",
      "using loss squared_error,  penalty is l1,  alpha is 1, the acc is 0.658\n",
      "[[228  73]\n",
      " [128 158]]\n",
      "0.6575809199318569\n",
      "using loss squared_error,  penalty is l1,  alpha is 5, the acc is 0.777\n",
      "[[286  15]\n",
      " [116 170]]\n",
      "0.7768313458262351\n",
      "using loss squared_error,  penalty is l1,  alpha is 10, the acc is 0.733\n",
      "[[286  15]\n",
      " [142 144]]\n",
      "0.7325383304940375\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is l2,  alpha is 0.1, the acc is 0.426\n",
      "[[219  82]\n",
      " [255  31]]\n",
      "0.42589437819420783\n",
      "using loss squared_error,  penalty is l2,  alpha is 1, the acc is 0.407\n",
      "[[190 111]\n",
      " [237  49]]\n",
      "0.4071550255536627\n",
      "using loss squared_error,  penalty is l2,  alpha is 5, the acc is 0.394\n",
      "[[120 181]\n",
      " [175 111]]\n",
      "0.393526405451448\n",
      "using loss squared_error,  penalty is l2,  alpha is 10, the acc is 0.451\n",
      "[[ 43 258]\n",
      " [ 64 222]]\n",
      "0.4514480408858603\n",
      "using loss squared_error,  penalty is None,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_error,  penalty is None,  alpha is 0.1, the acc is 0.491\n",
      "[[  4 297]\n",
      " [  2 284]]\n",
      "0.4906303236797274\n",
      "using loss squared_error,  penalty is None,  alpha is 1, the acc is 0.583\n",
      "[[136 165]\n",
      " [ 80 206]]\n",
      "0.5826235093696763\n",
      "using loss squared_error,  penalty is None,  alpha is 5, the acc is 0.693\n",
      "[[292   9]\n",
      " [171 115]]\n",
      "0.6933560477001703\n",
      "using loss squared_error,  penalty is None,  alpha is 10, the acc is 0.741\n",
      "[[286  15]\n",
      " [137 149]]\n",
      "0.7410562180579217\n",
      "using loss huber,  penalty is l1,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 0.1, the acc is 0.581\n",
      "[[299   2]\n",
      " [244  42]]\n",
      "0.5809199318568995\n",
      "using loss huber,  penalty is l1,  alpha is 1, the acc is 0.613\n",
      "[[171 130]\n",
      " [ 97 189]]\n",
      "0.6132879045996593\n",
      "using loss huber,  penalty is l1,  alpha is 5, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l1,  alpha is 10, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss huber,  penalty is l2,  alpha is 0.001, the acc is 0.853\n",
      "[[243  58]\n",
      " [ 28 258]]\n",
      "0.8534923339011925\n",
      "using loss huber,  penalty is l2,  alpha is 0.1, the acc is 0.642\n",
      "[[248  53]\n",
      " [157 129]]\n",
      "0.6422487223168655\n",
      "using loss huber,  penalty is l2,  alpha is 1, the acc is 0.618\n",
      "[[286  15]\n",
      " [209  77]]\n",
      "0.6183986371379898\n",
      "using loss huber,  penalty is l2,  alpha is 5, the acc is 0.598\n",
      "[[192 109]\n",
      " [127 159]]\n",
      "0.5979557069846678\n",
      "using loss huber,  penalty is l2,  alpha is 10, the acc is 0.608\n",
      "[[165 136]\n",
      " [ 94 192]]\n",
      "0.6081771720613288\n",
      "using loss huber,  penalty is None,  alpha is 0.001, the acc is 0.882\n",
      "[[273  28]\n",
      " [ 41 245]]\n",
      "0.8824531516183987\n",
      "using loss huber,  penalty is None,  alpha is 0.1, the acc is 0.787\n",
      "[[278  23]\n",
      " [102 184]]\n",
      "0.787052810902896\n",
      "using loss huber,  penalty is None,  alpha is 1, the acc is 0.651\n",
      "[[258  43]\n",
      " [162 124]]\n",
      "0.6507666098807495\n",
      "using loss huber,  penalty is None,  alpha is 5, the acc is 0.620\n",
      "[[262  39]\n",
      " [184 102]]\n",
      "0.6201022146507666\n",
      "using loss huber,  penalty is None,  alpha is 10, the acc is 0.615\n",
      "[[284  17]\n",
      " [209  77]]\n",
      "0.6149914821124361\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.736\n",
      "[[293   8]\n",
      " [147 139]]\n",
      "0.7359454855195912\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.554\n",
      "[[299   2]\n",
      " [260  26]]\n",
      "0.5536626916524702\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.624\n",
      "[[285  16]\n",
      " [205  81]]\n",
      "0.6235093696763203\n",
      "using loss epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.627\n",
      "[[253  48]\n",
      " [171 115]]\n",
      "0.626916524701874\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.722\n",
      "[[149 152]\n",
      " [ 11 275]]\n",
      "0.7223168654173765\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.535\n",
      "[[300   1]\n",
      " [272  14]]\n",
      "0.534923339011925\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.615\n",
      "[[289  12]\n",
      " [214  72]]\n",
      "0.6149914821124361\n",
      "using loss epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.598\n",
      "[[296   5]\n",
      " [231  55]]\n",
      "0.5979557069846678\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.596\n",
      "[[267  34]\n",
      " [203  83]]\n",
      "0.596252129471891\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.666\n",
      "[[299   2]\n",
      " [194  92]]\n",
      "0.666098807495741\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.770\n",
      "[[288  13]\n",
      " [122 164]]\n",
      "0.7700170357751278\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.729\n",
      "[[284  17]\n",
      " [142 144]]\n",
      "0.7291311754684838\n",
      "using loss epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.664\n",
      "[[284  17]\n",
      " [180 106]]\n",
      "0.6643952299829642\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 0.1, the acc is 0.499\n",
      "[[  9 292]\n",
      " [  2 284]]\n",
      "0.4991482112436116\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 1, the acc is 0.330\n",
      "[[ 17 284]\n",
      " [109 177]]\n",
      "0.33049403747870526\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 5, the acc is 0.760\n",
      "[[288  13]\n",
      " [128 158]]\n",
      "0.7597955706984668\n",
      "using loss squared_epsilon_insensitive,  penalty is l1,  alpha is 10, the acc is 0.784\n",
      "[[289  12]\n",
      " [115 171]]\n",
      "0.7836456558773425\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 0.1, the acc is 0.566\n",
      "[[149 152]\n",
      " [103 183]]\n",
      "0.565587734241908\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 1, the acc is 0.625\n",
      "[[179 122]\n",
      " [ 98 188]]\n",
      "0.6252129471890971\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 5, the acc is 0.399\n",
      "[[105 196]\n",
      " [157 129]]\n",
      "0.3986371379897785\n",
      "using loss squared_epsilon_insensitive,  penalty is l2,  alpha is 10, the acc is 0.382\n",
      "[[ 90 211]\n",
      " [152 134]]\n",
      "0.38160136286201024\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.001, the acc is 0.513\n",
      "[[301   0]\n",
      " [286   0]]\n",
      "0.5127768313458262\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 0.1, the acc is 0.496\n",
      "[[ 10 291]\n",
      " [  5 281]]\n",
      "0.4957410562180579\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 1, the acc is 0.647\n",
      "[[263  38]\n",
      " [169 117]]\n",
      "0.6473594548551959\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 5, the acc is 0.763\n",
      "[[262  39]\n",
      " [100 186]]\n",
      "0.7632027257240205\n",
      "using loss squared_epsilon_insensitive,  penalty is None,  alpha is 10, the acc is 0.787\n",
      "[[253  48]\n",
      " [ 77 209]]\n",
      "0.787052810902896\n"
     ]
    }
   ],
   "source": [
    "#SGDClassification\n",
    "X=dfFinal.drop(columns = ['AUTHOR','CONTENT','CLASS','A_SPEC','love','views','song'])\n",
    "y=dfFinal['CLASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "losses = ['hinge', 'log_loss', 'log', 'modified_huber', \n",
    "          'squared_hinge', 'perceptron', 'squared_error',\n",
    "          'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalties = ['l1','l2',None]\n",
    "alphas = [0.001, .1, 1, 5, 10]\n",
    "for _loss in losses: \n",
    "    for _penalty in penalties:\n",
    "        for _alpha in alphas:\n",
    "            model = SGDClassifier(loss=_loss, penalty=_penalty, alpha=_alpha, random_state=10)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            print(\"using loss {}\".format(_loss) + \",  penalty is {}\".format(_penalty) +\n",
    "                  \",  alpha is {}\".format(_alpha) + \", the acc is {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "            print(confusion_matrix(y_test, pred))\n",
    "            print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "230204b6-63a2-4776-9e83-75a49005d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[258  43]\n",
      " [ 30 256]]\n",
      "0.8756388415672913\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier:\n",
    "model = DecisionTreeClassifier(random_state = 10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bdc1a414-ec78-4407-9672-2e78d469cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[274  27]\n",
      " [ 31 255]]\n",
      "0.9011925042589438\n"
     ]
    }
   ],
   "source": [
    "#SVC with kernel=linear and C=1 had highest acc\n",
    "model = SVC(kernel='linear', C=1, probability = True, random_state=10)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "af74f02a-5fe1-4fae-907c-a2088008d2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code taken from https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "#Generating ROC and AUC for Linear & C=1\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC / AUC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1810124-9ce5-4240-952c-68ceab91483f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
